---
layout: post
title:  "Information and Meaning"
---


## TODO

## Escaping the Matrix

In Western thought we tend to think of the body as a second-class entity, as the thing being controlled by the mind;
a tradition that started with Plato continued with Descartes and the scientific revolution.
There is this longing to overcome the flesh

Plato wants to guide us out of the cage and into the light of wisdom.
He wants us to recognize the illusion directly in front of us.
Paradoxically, what he initiated led to the contruction of an actual illusion bringing metaphysics into our world.

Contrary to Plato's belief, phenomenology suggest that there was never a cage in the first place.
To escape the Matrix, it is unnecessary to swallow the "red pill".
Instead, turn off your phone -- your limitless distractions -- and go out to touch, see, and be present in the world.
We must embrace being a perpetual beginner: coming back again and again to what we perceive before us, remaining open to the adventures of experience.

Even if we have this attitude, our life is not a series of problems to be solved through sophisticated analysis of data or facts to be recorded.
For example, mindfulness practice to improve ones performance or other "life hacks" are just another instrumentalization of being.
Meditation for the sake of a better life-work-balance is just mental workout.


Being is bigger than that.

While science and technology liberated many of us from our own ignorance and the harsh condition of a life with nature, today it is hard to believe that more knowledge, more information, and more technology will continue this trend if we 


When it comes to specific scientific investigations there is nothing wrong about this approach.
But as a general position on how things are it is problematic, since our immidiate experience of the world is very different.

We never need to look for our right arm in the way we might look for a pair of scissors around our desk.
That cloud that I see can be a small cloud close overhead or a huge cloud far above; 
meanwhile what I had thought was a bird turns out to be a speck of dust on my glasses.

>I cannot conceive myself as nothing but a bit of the world, a mere object of biological, psychological or sociological investigation. -- Merleau-Ponty

The worldlyness of our *being-there* combined with our subjective and inter-subjective experience enriches our increasingly "head-based" and digitial reality.
We should allow ourselves to wonder and neglect any rationalization that reduces being to an object.


Heidegger posses the question if our objectifying mode of thinking led us into a vortex that we can no longer escape if we stay on the path of domination.


## Criticism of Heidegger

As I already disclaimed, Heidegger is a dangerous thinker and we have to be aware of his connection to *national socialism*.
He is so effective and appealing because he hit a lot of hight notes.
His emphasis on experience through the lense of *Dasein* as *being there thrown* into a world already in progress and his understanding of technology as a *mode of understanding* are powerful sources of critique of modern life and our technological development under *technology exceptionalism*.
He expresses a mood that many of us feel: that we instrumentalise ourselves more and more.

## AI and Heidegger

For scientists it is rather senseless to take Heidegger's philosophy seriously since it suggest to stop doing science all together.
Heidegger's philosophy suggests limits to the scientific method that we cannot surpass.
He famously claimed that

>Science does not think.

By which he means science can not understand the being-in-the-world, which might be the reason for Dreyfus's neglaction.

## Thinking in Symbols?

On the one hand, computers can only do symbolic manipulations, i.e., they compute (in a discrete world).
They change zeros to ones and vice verca but they are not aware of what they do.
If you ask ChatGPT: Are you sad?
It will clarify that it is unable to be in any mood; it only pretends to be sad.
It simulates the communcation of a person that is in a bad mood.

On the other hand, achievements in the area of machine learning suggest that they large networks preceive in a similar way than humans.
These networks do not measure the length of the edges to decide wether an image depicts a chair or a desk.
Instead they *learn* a model of their underlying reality, all its fuzzyness, and then "just know" that the picture depicts a chair.

But let us start from the beginning.
For this we have to go back to a time of pioneers of AI, a time before the rise of *machine learning*.
At that time, many thought that human-like intelligence can be thought of the ability of playing chess, i.e., they belied in *computationalism* (the mind operates like a Turing machine).
If we ignore the chessboard and only care about the moves of chessplayers, then chess is a purely symbolic game.
There are countable many chess games, therefore, in theory, a computer can compute the best next move at any stage of a game.
This leads to the conclusion that thought is explicitly symbolic which is supported by Descartes *mind-body dualism* (mental phenomena are non-physical).
In that case, we just need the correct mathematical formula or some algorithm and a symbol processing machine to reproduce human thinking.
Cognition is the manipulation of internal (explicit) symbols by internal rules thus human behaviour is, to a large extent, context free.
In other words: we just have to look inside the box (the human brain) to fully understand the actions of a person.
Dreyfus' critique of AI concerns this assumption that the brain is analogoues to computer hardware and the mind is analogoues to computer software and that the mind works by performing discrete computations (in the form of algorithmic rules) on discrete representations or symbols.

Dreyfus uses Heidegger's understanding of being that reverses this assumption.
Heidegger argues that our being is in fact highly context-bound.
We are a form of being-in-the-world that is aware of its and other's being-in-the-world.
A person and its world can not be seperated the subject-object distinction is fuzzy.
For example, if I leave a note at a desk, then this physical note is an extension of myself.
Parts of my memory lies on the desk at home.
Furthermore, it is a huge leap from choosing to see reality as consisting of indivisible atoms to state that because we want to or can see things in this way that it is therefore an objective fact that they are the case.
He aggressivly stated that any research program that assumes that such "facts" are real facts will quickly run into prfound theoretical and practical problems.
To think like humans, a machines would require to being-in-the-world like humans are.
Therefore, they would need the awareness of their own and other's being-in-the-workd which leads to the concept of not-being, i.e., death.

*Husserl*, *Heidegger* and *Merleau-Ponty* reject the kind of scientific naturalism or "*scientism*" that takes empirical science to be *epistemologically* and *metaphysically* privileged over all other forms of enquiry.
However, this does not make phenomenologists *ant-science* and  
Therefore, I will first try to give you the back bone 

### Alchemy and AI

In the summary of his controversal publiction Dreyfus states:

>An examination of [the difficulties to simulate cognitive processes on computers] reveals that the attempt to analyse intelligent behavior in digital computer language systematically excludes three fundamental huam forms of information processing (fringe consciousness, essence/accident discrimination, and ambiguity tolerance). Moreover, there are four distinct types of intelligent activity, only two of which do not presuppose these human forms of information processing and can therefore be programmed. -- Dreyfus in Alchemy and Artificial Intelligence



**Prompt:** Imagine you are a robot that can walk like a human. Now imagine I ask you to buy and bring me the best pizza to eat. What would you do?

**Answer:** As a highly advanced AI language model, I don't have a physical body to walk or perform tasks. However, if I were a robot with human-like walking capabilities, here's how I would fulfill your request:

Research: I would first search for the best pizza places in your area, taking into consideration customer reviews, ratings, and menu offerings. I would prioritize establishments known for their fresh, high-quality ingredients and unique flavors.

Decision-making: After analyzing the available options, I would select the most highly recommended pizza place that meets your preferences.

Ordering: I would contact the chosen pizza place to place an order for the best-rated pizza on their menu, ensuring it meets your dietary restrictions, if any. If allowed, I would also customize the pizza with toppings you prefer.

Payment: I would pay for the pizza using a pre-authorized method, such as a credit card or digital wallet.

Picking up the pizza: I would walk to the pizza place, maintaining a human-like gait and adhering to traffic and pedestrian rules.

Transporting the pizza: I would carefully carry the pizza box, making sure it remains level and undisturbed during the journey back to you.

Delivering the pizza: Upon reaching your location, I would present the pizza to you with a friendly greeting and ensure you are satisfied with the choice.

Cleanup and follow-up: Once you have finished enjoying the pizza, I would assist in cleaning up, if necessary, and ask for your feedback on the experience. This would help me better understand your preferences and make better choices in the future.







### Causal Theory of Intentionality

One naturalistic theory of intentionality is the *Causal Theory of Intentionality*.
The idea is that a thought or idea is about something if it is caused by that thing, i.e., there is a causal connection between that mental state and the thing it's about.

This sounds pretty plausabil.
If I see a cat then my thought is about the cat because the cat caused my thought.
I connect the *cause* (seeing the cat) by the *effect* (my thought of the cat).

This theory is appealing but has several problems.
Firstly, it struggles to address misrepresentations.
For instance, you may have a thought about your cat, but in reality, you might be mistaken, and the thought is about your neighbour's cat.
Maybe it's night, and the visibility is poor, leading to this misperception.
In this scenario, your thought about your cat is caused not by your cat, but by the neighbour's cat.
If both can trigger the same effect, wouldn't the effect differ, leading to a thought about "your cat and the neighbour's cat"?

Another issue is the problem of hallucination.
Your thought about your cat might not be triggered by the cat itself but by a hallucinogenic drug or a simulation, such as the Matrix.

The theory also fails to explain how one can harbor a thought about something that does not exist, like a blue elephant.

Finally, the theory is confronted with a degree of arbitrariness.
The causal chain from the stimulus that instigates the thought to the brain processes is quite extensive.
The light photons travel from the sun, hit the snow on Mount Everest, travel through the air into the eyes, and so forth.
There are no objectively defined endpoints and no rationale as to why the light from Everest and our thought should be the two endpoints.

### Biological Theory of Intentionality

The basic idea of the *Biological Theory of Intentionality* is that our cognitive capacities have evolved for certain purposes over millions of years, and these purposes determine the content of our intentional states (the states that are about something).
This theory suggests that intentionality isn't just about the relationship between your thoughts and the world, but also about the biological processes that give rise to those thoughts.

The need to run away from predators is a biological function and perhaps over time this function becomes a representation of predators.

This theory avoids the misrepresentation problem.
In this case your thought that bears represent bears is caused not by bears themselves but by the desire to avoid predators.
Therefore, if you mistakenly believe that an old stump is a bear this can be explained as a result of your representation being caused by the desire to avoid predators.

One problem of this theory is that it can not account for sophisticated and abstract beliefs and thoughts concerning math, philosophy and metphysical reasoning and so forth.
Spaceships or subatomic particles have no relevance to our evolutionary history thus it is unlclear how this theory can account for that.

Furthermore, the theory can not explain the either-or-problem.
If your desire to avoid both bears and stumps that look like bears has been wired in by evolution, then your thought that represents bears would actually represent "bears or stumps that look like bears" and not "bears" specifically.

### Connectionist Theory of Intentionality

The *Connectionist Theory of Intentionality* is a perspective that "aboutness" is based on the framework of connectionism.

Connectionism models cognition using *artificial neural networks* or systems that mirror the structure and function of neural networks in the brain.
The system's state (the pattern of activation across the nodes) can represent a particular object or concept if the pattern has been learned over time in response to that object or concept.
For instance, a certain pattern of activation could represent a cat if the system has learned to activate that pattern when a cat is present.

This approach allows for a nuanced understanding of mental representation, as it can account for properties like gradience (degrees of activation), distributed representation, parallel processing, and the ability to handle noisy or incomplete data.

However, the *Connectionist Theory* is primarily a model of how cognitive processes could work in the brain or in *artificial neural networks*, rather than a full-fledged philosophical theory of intentionality.
The theory does not explain how the patterns of activations associated with certain inputs have semantic content, i.e., how they come to be "about" something in the world.
They are not *grounded* in experience!
Similar to any computation we humans, i.e., external observers, interpret the output and states of the system.

Another problem is the distribution of knowledge across the network.
This makes it difficult to account for our ability to reflect on our knowledge and beliefs, which seem to require explicit representations.
It is also unclear how normative aspects of intentionality (our thoughts can be correct or incorrect, appropriate or inappropriate) can be acoounted for.

### Phenomenal Intentionality Theory

All the above theories are *naturalistic* theories of intentionality.
These theories attempt to *ground* intentionality in the *physical world* and often align with a *materialist* view of the mind.

The *Phenomenal Intentionality Theory (PIT)* posits that intentionality arises from conscious experience or phenomenology, i.e., is *grounded* in or derived from the phenomenal character of experience.
The contents of our thoughts -- what our thoughts are about -- are determined by the way things subjectively appear to us in our conscious experiences.
Therefore, *intentionality* plays a fundamental role for consciousness that can't be reduced to or explained in terms of more basic physical or natural processes.

PIT is not traditionally considered a naturalistic theory of intentionality, but this is debated.
If one finds a naturalistic explanation for consciousness, then PIT might be compatible with *naturalism*.

PIT takes a "consciousness first" approach to intentionality, since it claims that consciousness either grounds or is explanatorily prior to intentionality.
For example, someone who accepts *phenomenal intentionality* might say that if you see a red cube and have a thought about that cube, then your thought is "about" the cube because of the way the cube appears to you in your conscious experience.
It's not just that you're receiving sensory information about the red cube, but the way you consciously experience the cube shapes what your thought is about.
Therefore, *intentionality* is not just a mechanical or biological process, but is deeply tied to our subjective experiences.

PIT also faces challenges such as explaining how unconscious thoughts can have intentionality, how we can have thoughts about abstract or non-existent entities, and how to reconcile the subjective nature of phenomenal experience with a more objective, scientific understanding of the world.

## MISC

In the face of the biggest challenge we as a species might ever have wittnessed, we need hope, we need to act and many studies assume that AI will be part of the solution.
Furthermore, this crisis opens up a new horizon of the possible.
Because whole systems and our way of life have to be transformed there is suddenly room to dream of a better world -- *cyberpunk* became *solarpunk*.

In our economically driven system, the job we do highly influence our monitary as well as social capital which translate to a value we assign to people.
We preach humanistic values and a Kantian notion of each individual being an end in themselve.
However, self-esteem is not an essential property but emerges from constant feedback within social systems (family, workplace, institutions).
We become how we are preceived.


## Dreyfus and AI

Hubert L. Dreyfus (1929 - 2017) was a philosopher deeply influenced by and interested in phenomenology and existentialism.
He studied the works of *Edmund Husserl*, *Martin Heidegger*, *Soren Kierkegaard*, *Maurice Merleau-Ponty*, *Jean-Paul Sartre*, *Nietzsche* and *Dostoevsky*.

Dreyfus held professorial positions at *MIT* and later at the *University of California*, *Berkeley*.
During the 1960s to the 1980s, he emerged as a staunch critic of AI, which led to tensions at MIT, nearly costing him his job and drawing personal attacks from the AI community.
His 1965 publication *Alchemy and Artificial Intelligence*, in which he compared AI to the process of alchemy trying to make gold from metal, sparked considerable controversy.

In following 1972 book, *What Computers Can't Do*, which essentially expands on his earlier publication, Dreyfus delves into the inherent inability of disembodied machines to replicate higher mental functions.
In the follow-up book, *What Computers Still Can't Do: A Critique of Artificial Reason*, he provided a comprehensive new introduction, addressing recent developments in AI and evaluating the paradigms of connectionism and neural networks that have reshaped the field.

In the end, Dreyfus was both right and perhaps wrong simultaneously.
At a time when researchers were proposing ambitious plans for general problem solvers and automatic translation machines, Dreyfus predicted their failure due to their naive understanding of mental functioning as a purely symbolic activity.
He recommended that they should familiarize themselves with contemporary philosophical approaches to human cognition.
Probably due to his aggressive attitute he got completely ignored -- for the better or worse.

Today, many connectivsts will argue that formal methods are not expressive enough for human-like cognition and indirectly agree with Dreyfus.
It seems that he was correct in asserting that perception and cognition are not pure symbolic powered expert systems.
However, he may have been wrong in suggesting that symbol manipulation cannot simulate the fuzziness of cognition and unconscious fast thinking we use when catching a ball.

As a fierce critic, Dreyfus had to endure harsh criticism of his own work.
He noted that 

>My colleagues working in AI dared not be seen having lunch with me.

Joseph Weizenbaum, the famous inventor of *ELIZA*, the first chat bot that was able to simulate human-like conversations, was an outspoken critic of Dreyfus' position but he took it professional.

>I became the only member of the AI community to be seen eating lunch with Dreyfus. And I deliberately made it plain that theirs was not the way to treat a human being. -- Joseph Weizenbaum

Seymour Papert organized a chess match between Dreyfus and *Mac HackÜ (a computer program), which Dreyfus lost, much to Papert's delight.

>"A ten year old can beat the machine" Dreyfus says. Well, the machine beats Dreyfus. What does this tell us about him?

By the early 1990s, several of Dreyfus' radical opinions had become mainstream. 
However, the AI community largely continued to ignore Dreyfus, since the gap between their principles of naturalism and Dreyfus' phenomenology remained too vast.

>What does he offer us? Phenomenology! That ball of fluff. That cotton candy! -- Edward Feigenbaum

Moreover, Dreyfus was not inclined to address the problems he identified but instead provided an absolute answer: it will never happen.
Because of the advances in cognitive science the gap has closed a lot lately.
Many phenomenological claims are now founded in cognitive science but some of the more fundamental criticism is ignored in mainstream discussions about *artificial intelligence* and its impact on society.

Because this topic is so fundamental, I want to do it justice.
Therefore, I will try to guide you into the phenomenological mindset.
We all know naturalism.
It is how we imagine the reality to be, i.e., the assumption that nature is best accounted by reference to material principles, that it encompasses all that exists throughout space and time.
So what is *Phenomenology*?




## Musical Interrogation



#### Generation of Performances

All the above models could only learn a rather simplstic note sheet.
These techniques neglect timing and dynamics (changing tempo, micro rests, softness or velocity key presses) within a performance.
The following models learn a performance directly, i.e., without first generating a score.

[DeepMind](https://www.deepmind.com/) with their paper *This Time with Feeling: Learning Expressive Musical Performance* {% cite oore:2018 %} tried to change this flaw.
The explanation in their paper is refreshinglgy extensive.
They give insights in the musical aspects of their choices.
It is a great read.
Their LSTM consists of three layers of 512 cells each.
At each time step, the input of the RNN is a single one-hot 413-dimensional vector (128 note-on, 128 note-off, 125 time-shift, 32 velocity).
Interestingly, time-shift events move the time step forward by increments of 8 ms up to 1 second, i.e., the authors uses representation that is temporally non-uniform!
They need such a representation to be able to have different temporal resolutions such that the model is able to learn micro variations without an explosion in the input and output length.
It is a kind of compression.
Holds or rests can be skipped with a single event.
The authors also experimented with a pedal-on/off event.
There are inspired implementation on [GitHub](https://github.com/igor4149/Sonia) but I do not know jet how accurate they are.
However, with their technique the authors were only able to achive good results for pieces no longer than approximately 15 seconds (500 tokens).

Another LSTM is the [Performance RNN](https://github.com/magenta/magenta/tree/main/magenta/models/performance_rnn) {% cite simon:2017 %}.
It is  also developed as part of the Magenta project with a focus on polyphonic music with expressive timing and dynamics.
They use a similar input compared to {% cite oore:2018 %} and with respect to the code it seems to be a three layer LSTM network with 512 units in each layer.
It seems to be almost the same model, after all, DeepMind belongs to Google.

Another performance generator is the [Music Transformer](https://github.com/magenta/magenta/blob/main/magenta/models/score2perf/README.md) {% cite huang:2018 %}, again part of the Magenta project.
You can listen to the transformer [here](https://magenta.github.io/listen-to-transformer/#a1_83641.mid) and there is also an [interactive Colab notebook](https://colab.research.google.com/notebooks/magenta/piano_transformer/piano_transformer.ipynb) where you can play with the model.
The code is concealed via abstraction but [Ki-Chang Yang](https://github.com/jason9693/MusicTransformer-tensorflow2.0) re-implemented it without all the Magenta dependecies.
The main author, [Cheng-Zhi Anna Huang](https://czhuang.github.io/), is also involved in other very interesting projects such as helping with the [AI song contest](https://www.aisongcontest.com/) but I digress.
*Music Transformer* isn't an LSTM!
Instead it is a transformer that uses a **relative local attention mechanism** introduced in {% cite luong:2015 %} and {% cite shaw:2018 %} respectively.

Transformers benefit from their ability to directly access all events happened in the past (without going step by step back in time).
RNNs and their extensions (LSTMs, GRUs) have a limited time span in practice while transformers do not suffer from this shortcomings (if you have enough computation power and memory).
Basically every symbol/token pays different amount of attention to all the other symbols within the sequence.
Different to an RNN, the transformer requires an explicit position information since it has no implicit representation of time -- it looks at the whole sequence at once.

In musical composition and performance, relative timing is critically important.
Therefore, using relative instead of absolut positional information seems a good idea.
By using *relative positions*, the authors try to capture relative timing and optionally also pitch within a piece.
However, for relative positional information one requires $$L$$ relative positions for each token of an input sequence of length $$L$$.
This yield $$\mathcal{O}(L^2)$$ many relations which is a problem if sequences get longer!
Let's say you want to generate a minute-long fine grained song with 2000 tokens.
This would yield 4 Million realations!
The *Music Transformer* uses *local attention*, meaning a sequence is split into blocks and each block can only pay attention to itself and direct neigbhours.
They also use a smart implementation trick to reduce the space complexity (while the time complexity remains the same).
This is important to bring the computation to the GPUs.
Consequently, the authors were able to generate thousands of steps, four times the length modeled achieved by {% cite oore:2018 %}.
According to the results in {% cite huang:2018 %}, *relative attention* achieves a better quality compared to *vanilla transformers* and other architectures.
A single GPU with 16 GB is able to handle a maximal training length of 3500 tokens.

# TODO:

{% cite huang:2020 %}

[MuseNet](https://openai.com/research/musenet) (from OpenAI) which uses the same technology as GPT-2, i.e. a large-scale transformer.
72-layer network with 24 attention heads—with full attention over a context of 4096 tokens.