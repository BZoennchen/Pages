---
layout: post
title:  "Machines Do Not Think"
tags: AI Philosophy Consciousness
---

In January 2015, Stephen Hawking, Elon Musk, and dozens of artificial intelligence (AI) experts signed an open letter on AI, calling for research on the societal impacts of AI. 
The letter highlighted both the positive and negative effects of AI, but warned that if human intelligence is achieved, the risks could be unpredictable. 
This year, a second open letter, signed by Elon Musk, Steve Wozniak, Yuval Noah Harari and others, followed.
The authors call for a six-month pause in developing systems more powerful than OpenAI's newly launched GPT-4.

>Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable

said the letter issued by the *Future of Life Institute*.
This statement reveals two assumptions which are problematic if they are taken for granted.
First, the authors claim that we can be confident that the effects of powerful AI systems are positive if we only analyse those systems enough, assuming that technology is a mere tool that we (humans) can control. 
Second, the letter attributes a power to AI systems that we as a society have to respect.
In combination the authors imply that AI is powerful and will definitely be part of the life of a vast amount of people.
The warning thrusts AI deeper into our collective consciousness.

AI research is in a state of uncertainty.
Experts can build machines they do not understand and that at least simulate some sort of *calculated rationality (CR)*.
Furthermore, we do not know what human-like intelligence truly is or how consciousness works. 
Yet at the same time, disruptive machines that seem to exhibit intelligence pop up in our daily life.

The reaction of ordinary people to the *uncertain state of AI* combined with the horrific statements about its impact causes

1. shameless optimism, 
2. fear regarding job security, and 
3. an existential anxiety

and I too feel all these emotions, sometimes simultaneously.

My issue with the mainstream discourse surrounding AI is that it is mainly led by certain people with a particular belief system which confuses *conscious thinking* with *artificial intelligence*.
They largely assume that the human brain is fundamentally a *Turing machine* and that consciousness is *Turing computable*.
Their shared belief can be explained psychologically.
By extrapolating from the biased sample of people-you-spend-time-with, you fall into the trap to believe that your hypothesis are much more accepted than they are;
If you hold a hammer in your hand everything becomes a nail.

If the mind is *Turing computable*, the leap towards human-like thinking machines is not a big one.
Therefore, it is no wonder why so many futurists come out with quite radical claims, especially if they are non-experts in the field of machine learning.
For instance, Elon Musk (entrepreneur) states:

>With artificial intelligence, we are summoning the demon. You know all those stories where there's the guy with the pentagram and the holy water, and he's like, yeah, he's sure he can control the demon? Doesn't work out. -- Elon Musk

Ray Kurzweil (author, inventor, futurist) said that

>artificial intelligence will reach human levels by around 2029. Follow that out further to, say, 2045, we will have multiplied the intelligence, the human biological machine intelligence of our civilization a billion-fold. -- Ray Kurzweil

And Nick Bostrom (philospher) stated that

>machine intelligence is the last invention that humanity will ever need to make. -- Nick Bostrom

The media echos this perspective and often portrays AI as an attempt to create a human-like entity -- powerful enough to overtake the world. 
This comparison sparks an existential question and has consequently the potential to make our day to day interactions less humane.
If conscious human thinking can be decoupled from us (whatever we are) our conception of ourselves and our fellow human beings will completely change.
And, considering our history of dehumanization, the outlook would likely be rather bleak.

Despite the provocative title, the intention of this article is not to convince the reader from the opposite or to rescue *humanism* from reasonable attacks but to critique the futuristic view; to peek holes into its logic.
My aim is to challenge the view that

1. the operations of latest *machine learning models*, such as *large language models (LLMs)* are similar to human thinking
2. technology is neutral.

In order to accomplish this, I delve into philosophical concepts.
I must admit, I do not possess formal training in philosophy.
My contributions are purely based on private reading and personal reflections.
Therefore, I strongly encourage you to remain critical as you read and take everything mentioned in this article with a grain of salt.

## The Hard Problem of Consciousness

Language can pose a complex challenge in a discussion, particularly in the field of *machine learning*, where we often rely on metaphors and somtimes ill-suited analogies.
There is a general human weakness for explainations of what is incomprehensible in terms suited for what is familiar and well understood, though entirely different.
In the tech sphere, we employ existing terms to describe new and frequently technical operations, processes, and entities.
For instance, one of the most influential publications in recent machine learning research is titled *Attention Is All You Need* {% cite vaswani:2017 %}. 
Here, *attention* serves as a metaphor for a complex computation involving the calculation of specific weights, which indicate how "important" one token/sub-word is to another, in other words, how much "attention" one token/sub-word devotes to another.
Even in this explanation, I cannot entirely circumvent the unconventional use of language. 
The challenge lies in the fact that a rigid description necessitates mathematics. 
Even though metaphors can be incredibly useful, they are not a perfect substitute for the real concept

>One cannot understand [...] the universality of laws of nature, the relationship of things, without an understanding of mathematics. There is no other way to do it. -- Richard Feynman

Each term carries a unique definition within the discipline of machine learning, which differs from our everyday language usage.
We encounter terms such as *learning rate*, *decision making*, *meta-learning*, *machine teaching*, *experience*, *task*, *transfer learning*, and, of course, *artificial intelligence*:

>In computer science, we refer to *information processing systems* as *artificial intelligence* when they resolve complex tasks assigned by humans, exhibiting skills usually associated with human intelligence.

Within philosophy and computer science, one differentiates between **weak** and **strong AI**.
The terms were coined by philosopher John Searle in 1980 as part of the *Chinese Room Argument* (explained below):

+ **Strong AI hypothesis**: An artificial intelligence system can "think" -- have "a mind" and "consciousness".
+ **Weak AI hypothesis**: An artificial intelligence system can (only) **act like** it thinks and has a mind and consciousness.

**Weak AI** implies *demonstrating capabilities*, that is, certain information processing systems seem intelligent; they simulate intelligence.
**Strong AI**, on the other hand, appears intelligent because it genuinely possesses intelligence. 
It understands, and hence, is *conscious* and/or exhibits *intentionality*.

The crux of most AI discussions boils down to this distinction, coupled with a misuse of language. 
People claim a system is intelligent (or creative) when they actually mean that it appears intelligent, rather than truly being so. 
Confusingly, Ray Kurzweil use the term **strong AI** to *indicate human-level artificial general intelligence*, which isn't the same unless one presumes *consciousness* is necessary for *human-level AGI*.
*Artificial General Intelligence (AGI)* refers to a hypothetical intelligent agent capable of learning any intellectual task performed by humans or other animals. 
The creation of AGI is a primary objective of some AI research and companies, including OpenAI, DeepMind, and Anthropic.
From an instrumental perspective, the consciousness of *AGI* doesn't matter as long as its impact on the world would remain consistent. 
However, when considering ethical, philosophical, and anthropological viewpoints, the question of *consciousness* becomes paramount.
In {% cite bubeck:2023 %} the authors believe GPT-4

>that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.

Some argue that **strong AI** is achievable, while others disagree.
While it is important to think about the topic, one should avoid claiming the advent of **strong AI** without proof.
Searle, a *materialist*, doesn't refute the possibility of constructing **strong AI**. 
However, he insists that such a machine would need *intentionality*, meaning there are states that involve semantic content referencing objects, properties, or relationships beyond themselves (*grounding*).

Philosopher Daniel Dennett posits in {% cite dennett:1991 %} that consciousness isn't what it seems, but rather a series of mental activities creating an illusion of a unified, continuous self. 
Consequently, if consciousness is a process or simulation, there's no fundamental barrier to replicating that in a machine, given sufficient understanding and computational resources. 
Dennett argues that "beliefs", "desires", or even "consciousness" itself don't correspond to real *phenomena* and should be substituted with more accurate concepts based on neuroscientific comprehension.

David Chalmers {% cite chalmers:1995 %} and Thomas Nagel {% cite nagel:1974 %} argue that Dennett's model ails to account for the subjective experience of consciousness, also known as *qualia*. 
There is something it "feels like" to have a particular conscious experience, something that cannot be fully explained by a physicalist or reductionist view.
We can measure the brain function and observe correlations between firing neurons and human behaviour but this tells us nothing about the fact that there is a quality to be human -- I feel like a certain way.
Why is this the case?
This problem is *The Hard Problem of Consciousness* (further discussed below).

Proponents of **strong AI** often think that as simulated intelligence improves, it somehow becomes conscious. 
The assumption is that the system becomes more than the sum of all parts (*emergence*), a plausible materialistic supposition influenced by our understanding of how consciousness emerges in our brains. 
Another possibility is that all things are somewhat conscious, with the level of consciousness gradually increasing. 
While I'm not denying these hypothesis categorically, there's no evidence that such *emergence* or *gradual transition* can occur.

Perhaps the endeavor to answer the question of **strong** versus **weak AI** is destined to be inconclusive since without peeking "into the box", we can never truly ascertain whether an agent is merely simulating or not;
These hypothesis may never be *falsifiable*.
The same holds true for intelligence, which teachers typically measure through testing students -- there's simply no other practical method for determining one's intelligence or knowledge.

In summary, the *question of AGI* should not be conflated with the question of **strong AI**.
The latter is intimately tied to *The Hard Problem of Consciousness*.
All those questions are open for debate and it is important to remember this when making bold claims about *artificial intelligence*.

Let's conclude this section with a quote from the physicist, cosmologist and machine learning researcher and president of the *Future of Life Institute* Max Tegmark, who believes he has considered all possible consequences of superintelligence for the next 10,000 years and that "the world is not merely described by mathematics, but actually made of mathematics":

>In essence, we cannot absolutely guarantee that within our lifetime there will be a human-like AGI, but we cannot rule out the possibility either. -- {% cite tegmark:2017 %}

## Phenomenology

*Phenomenology*, derived from the Greek word "phainomenon" (that which appears), is a philosophical study that explores the structure of raw experience and consciousness.
It was founded by the Austrian-German philosopher *Edmund Husserl* in an attempt to save our production of knowledge, i.e., to give it a solid fundament. 
He picked up the works of *Franz Brentano* who introduced the concept of *intentionality*.

>Knowledge is to be based upon empirical evidence not metaphysical and theological notions. -- Franz Brentano

Phenomenology was further developed by *Martin Heidegge*r, who wrote with *Being in Time (Sein und Zeit)* one of the most influential book of the 20th century.
He shifted the focus from *intentionality* and *consciousness* to the *nature of Being* itself -- from consciousness-oriented to world-oriented.
Later the french philosophers *Maurice Merleau-Ponty* and *Jean-Paul Sartre* added their contributions.

While all phenomologists emphasize the detailed description of conscious experiences or phenomena, it is a broad branch today.
Some phenomologists lean more towards *idealism* some towards *realism*, and many would argue that the traditional distinction between *idealism* and *realism* is not particularly helpful for understanding phenomenology.

One can argue that phenomenology was laid out by Kant's *Critique of Pure Reason* -- a project to explain how we combine what is known as sensory knowledge with reasoned knowledege.
Immanuel Kant argued that there is the *Thing-in-itself (Ding an sich)* which is the status of objects as they are, independent of perception.
Phenomena, in his view, are the things as they appear to us, while noumena are things as they are in themselves (Ding an sich).
Similar to phenomenologists, Kant also suggests that experience is actively structured by certain fundamental concepts.
But to perceive anything at all, some knowledge exists in the mind a priori (independent of experience) -- objects can be *intuited*, and that *intuition* is consequently distinct from the *Things-in-themselves*.
However, many phenomenologists do not posit the existence of noumena or things-in-themselves the same way Kant does.

Husserl admired and adopted Descartes' goal of finding an indubitable truth.
According to Husserl, if a philosopher wishes to start with the first evidence, they must not only exclude existing philosophy and science from their perspective, but also the world in general. 
The world is invalidated, and the philosopher focuses solely on their own consciousness and the **phenomena that this consciousness perceives**.
Husserl thought that there was a direct correlation between our experiences and the world around us.
As such, conscious experiences (phenomena) could provide a reliable basis for scientific knowledge.
In fact, he thought that Descartes' *rationalism*, was not radical enough and unlike Heidegger, he retained the *subject/object distinction* and the *transcendental ego*.

Husserl wanted a new solid foundation of knowledge and since it has to start with the phenomena, he wanted a *Science of the Human Spirit* (very similar to Decartes).
Husserl went back to Decartes and tried to bracket (suspending judgement) not the objects that appear but objects of knowledge (particulars, interpretations, and even memory).
In his later works he even tried to bracket his own theoretical attitude (metaphysical assumtions) to get to the "I" of the lived world (Lebenswelt) rather than the "I" of some abstruse theoretical knowledge of the world.
He wanted to get to the "I know" of a prescientific conciousness, that is, the "I know" of *ordinary life*.

Phenomenology re-emphasis the experienced phenomena over rational thinking and empirical evidence in so far that it values first-hand experimental data.
However, mainly it is not interested in tracing experiences back to an external, physical cause.
It goes against the *Representational Theory of Consciousness*, which assumes that conscious experience is fundamentally a matter of mental representation.
That is, our conscious experience of the world is a result of internal mental representations that depict or model the world.
This view is rather mainstream (cognitive science, neural processes) and compatible with recent developments in *artificial intelligence*. 
In contrast, phenomenology aims to provide a rich, descriptive account of our lived experience as it is, without reducing it to physical or psychological terms, or explaining it in terms of causal mechanisms.
There can be points of tension but also points of agreements here.
For example, phenomenologists emphasise the directness and immediacy of our experience which creates a tension with the view that experience is mediated by internal mental representations.

Phenomenology begins with the one thing we seemingly have access to -- our subjective experience -- and takes it very serious.
It emphasizes one's *world*, rather than focusing on an objective reality.
*World*, in this context, is what Germans mean when they say "meine Welt".
It is what we subjectively perceive.
It consists of moods, emotions and subjective perspectives.
One can say: "In my world everyone seem to be sad."
One of Heidegger's main point is that science (naturalism) has no access to this "world" as a whole, even though this "world" is intuitively understood by us.

Let's try to think phenomenological by using an example:
Rather than focusing on the measurable distance between one's home and the train station, phenomenology examines the subjectively felt distance.
Take time as another example.
If we study time objectively, we measure it in seconds, minutes, and hours.
This is the objective time that progresses at a constant rate, independent of human experience. However, the phenomenological perspective explores the subjective, first-person experience of time.

>Put your hand on a hot stove for a minute, and it seems like an hour. Sit with an interesting friend for an hour, and it seems like a minute.

For a rationalist or naturalist, time is consistent for everyone (with the exception of Einstein's relativity).
A phenomenologist recognizes that it can appear quite different.

Consider fear as another example.
A naturalist might analyze fear based on the physiological changes that occur, such as the activation of the sympathetic nervous system and increased heart rate, or by observable behaviors.
In contrast, the phenomenological perspective delves into what the actual experience of fear feels like -- the movements and dynamics of consciousness and how it influences the perception of the experience.

Lastly, let's examine Heidegger's most famous example: a hammer held in one's hand.
It is impossible to see the entire hammer at once since some parts are always concealed.
Nevertheless, one is certain of the existence of the object called "hammer".
Even though, scientifically speaking, there is no such thing as a hammer does not change the *truth* of the hammer.
Making a logical argument for the existence of the hammer would be peculiar.
While investigating the hammer, one doesn't continuously think, "I can see a hammer. I can still see a hammer.
Therefore, it is one object that is continually present."
Instead, one simply experiences the hammer in a world of objects without rationalization. 
Furthermore, the experience of the hammer is inseparable from recognizing its function, and properties.
It may be perceived as dirty, cheap or handy and if I see a hammer I also see its potential in hammering things.

>This world is not there for me as mere world of facts and affairs, but, with the same immediacy as a world of values, a world of goods, a practical world. Without further effort on my part I find the things before me furnished not only with the qualities that benefit their positive nature, but with value-characters such as beautiful or ugly, agreeable or disagreeable, pleasant or unpleasant, and so forth. Things in their immediacy stand there as objects to be used, the "table" with its "books", the "glass to drink from", the "vase", the "piano" and so forth [...] The same considerations apply of course just as well to the men and beasts in my surroundings as to "mere things". They are my "friends" or my "foes", my "servants" or "superios", "strangers" or "relatives" and so forth. -- Edmund Husserl

Phenomenology takes the phenomena of the hammer serious without trying to objectify and disect it into its pieces to capture the situated being in the world that each individual embodies.
This thinking is more *artistic* than *scientific*.
If you are at a concert, it makes no sense to determine the objective meaning of the event.
The real meaning, the truth, is what you experienced at the event.
In this context, objective reality is unable to provide "real truth".
Truth is what appears; what is presented to me.
Phenomenology offers an alternative perspective on truth that avoids emprirical or statistical investigations.
It is, however, not anti-science but trys to provide a kind of "first-person" perspective that can enrich our understanding of consciousness and experience in ways that complement the "third-person" perspective of the sciences.

While it is often feasible to adopt a viewpoint other than one's own, the understanding of such perspectives is not confined to one's personal experience.
There is a sense in which *phenomenological* facts are impeccably objective: one individual can discern or articulate the quality of another's experience. 
However, they are subjective in the sense that even this objective assignment of experience is only possible for someone sufficiently similar to the person to whom the experience is being ascribed; someone who can understand the ascription from both a first-person and third-person perspective, so to speak. 
The greater the divergence between the experiencer and oneself, the less likely one is to succeed in this endeavor.

## Intentionality

Husserl sought to establish an objective study of the subjective (*transcendental phenomenology*) by proposing a *theory of intentionality*.
He developed this idea starting with Franz Brentano's *intentionality*.

Today, *intentionality*, which translates from Latin to "to point at", remains a significant concern among the *philosphy of mind*, particularly the *nature of consciousness*, the *semantics of language*, and the *problem of representation*.
*Intentionality* is a concept used to describe the ability of the mind to form representations and thoughts about or towards the world.
In simple terms, *intentionality* is

>the characteristic of many mental states and events by which they are directed towards objects or situations in the world, or deal with them. -- {% cite searle:1983 %}

A common debate involves the *material* source of *intentionality* and *consciousness* and their relation.
This has given rise to the aforementioned *dualism*, as the mind appears to possess various properties absent in physics. 
However, *dualism* is a challenging concept to accept. 
As Princess Elisabeth eloquently queried in a letter to Descartes in 1643: How can something immaterial cause a material impact?

>I beg of you to tell me how the human soul can determine the movement of the animal spirits in the body so as to perform voluntary acts
>-- being as it is merly a conscious substance.
>For the determination of movement seem always to come about from the moving body's being propelled--
> to depend on the kind of impulse it gets from what sets it in motion, or again, on the nature and shape of this latter thing's surface.
> Now the first two conditions involve contact, and the third involces that the impelling thing has extension; but you utterl exclude extension from our notion of soul, and contact seems to me incompatible with a thing's being immaterial. -- Princess Elisabeth

From a physics standpoint, this would result in various problems, such as the violation of the *conservation law*.
However, if dualism fails, then there arise intriguing challenges to materialism concerning intentionality.

Consciousness, in Husserl's view, has the property of *intentionality* (*aboutness*, *meaning*);
In presentation, something is presented;
in judgement something is affirmed or denied;
in love, something is loved;
in hate something is hated;
in desire something is desired;
Consciousness does not merely carry around representations of objects like a bag of items or ideas but it makes objects present to us.
It is directed and constitutive and meaning-giving and ordering.
The very nature of my knowing constructs the overall situation; 
pulls it together for me;
Conciousness is not passive but active;

The concept of *intentionality* can be likened to the directional "pointer" that connects a signifier to its corresponding signified.
For instance, the term "cat" refers to a real-life cat.
Similarly, an image signifies its depicted subject, and a thought about Mount Everest directs itself towards the actual Mount Everest.
However, the concept of intentionality goes beyond being a mere pointer. 
This is illustrated by the fact that one can harbor contradictory thoughts about the same object or concept.
For instance, someone might hold both beliefs that "the morning star is shining" and "the evening star is not shining," unaware that both stars are, in fact, the same entity.

Furthermore, signifiers, including thoughts, words, or images, have the capacity to represent entities that lack a tangible existence in reality, such as a pink unicorn.
This suggests that the connection between the signifier and the signified is more intricate than a straightforward physical pointer.
This relationship entails a complex interplay of mental and linguistic processes that significantly influence our comprehension and interpretation of the world around us.

How is *intentionality* actualized when the signified either does not exist, or when it refers to multiple entities, such as in the case of the term "atom"?
In the realm of linguistics, and contrary to Plato's notion of ideas as eternally stable, Saussure argued that the relationship between the signifier and the signified is arbitrary or "unmotivated".
The existence of a physical object "out there" is not a necessity.
For instance, while the letters "c-a-t" spell out 'cat', they don't inherently embody the essence of "catness".
Similarly, the French term "chat" does not correspond identically to the English "cat" in terms of the meaning it generates, as "chat" carries different connotations in French.
In another example, the French word "mouton" encapsulates both "mutton" and a living "sheep", whereas English differentiates between the two.
These discrepancies underscore the complexity and the subjective nature of linguistic signification.

We humans apply meaning to otherwise meaningless matter.
For instance, a picture signifies its subject matter only because we assign meaning to it. 
Absent this human intervention, the picture would not inherently refer to anything; particles of ink have no inherent correlation with, for instance, mountains.
Computers operate on a similar principle, even though we often perceive them as continuously referring to objects or concepts.
However, it is once again human interpretation that assigns meaning to the computer's output.
Computers manipulate symbols, and within a specific programming language, these manipulations hold significance.
However, it's important to note that no symbol is *grounded* in a tangible experience.
The meaning is derived from the context provided by the specific language and the human interpretation of that context.

To contest the notion of machines possessing human-like intelligence, one could argue that they operate on symbols or combinations of symbols that are not grounded in experience. 
A rebuttal to this might suggest that in machine learning, machines do accumulate experience. 
They are fed with billions of data points. 
However, even if neural networks are somewhat capable of grounding their "states" in tangible experience, this experience is fundamentally different from ours. 
Another counter-argument may propose that humans understand concepts that are not grounded in tangible experience, for instance, unicorns. But this isn't entirely accurate, as such concepts are invariably abstracted from some phenomena, such as a horse and a horn.

Searle argues that without *intentionality* there is no *understanding* and since, according to him, computers have no *intentionality* they do not understand anything.
Understanding, in relation to machines, is therefore used only metaphorically, by humans transferring their own intentionality onto artificial creations. 
According to Searle, computers are merely the extended arm of our objectives, and that's why we find it natural to attribute intentionality to them metaphorically, but philosophically such things are of no consequence.

If we adhere to *materialism*, the philosophy asserting that everything, including thought, can be explained in terms of matter and physical phenomena, then my contemplation of Mount Everest can be reduced to a specific arrangement of electrons, neurons, dendrites, and the like. 
In essence, it is just physical particles and nothing more.
However, unlike a computer, there's no external entity assigning meaning to these particles. 
So, the question arises: how can these ostensibly meaningless particles point to something, without an external mind assigning meaning to them, as we do with computers?
If we suggest that a sub-process in the brain assigns meaning to our thoughts, it merely relocates the problem without genuinely resolving it. 
To this day, the quandary of how raw physical processes can embody meaning remains unanswered.

## Heidegger and the Nazis

In the following I want to talk about my interpretation of Martin Heidegger's understanding of the human condition and his view on technology.
But before we can talk about that I have to include an important disclaimer!

While discussing the philosophical contributions of Heidegger, it is essential to also acknowledge his association with *National Socialism*.
He actively endorsed this ideology and exploited it as a mechanism for acquiring control, influence, and power.
For instance, he employed the services of the Gestapo (the Geheime Staatspolizei) to sideline his colleagues.
In other instances, he helped students to move away from the danger.
Regrettably, Heidegger neither apologized nor distanced himself from his actions, nor did he acknowledge any wrongdoing.
Instead, he attempted to obfuscate and justify his actions, which is particularly disconcerting given the fact that both his mentor, Edmund Husserl, and his student (and romantic partner), Hannah Arendt, were of Jewish descent.

Moreover, many argue that it is impossible to separate Heidegger's politics from his philosophy -- they are intertwined, and I tend to agree, at least to some extent.
Thus, engaging with Heidegger's work can be a perilous undertaking, even if many of his ideas are incompatible with Nazism.
Perhaps it was his concern with technology that spiraled into fear and desperation, leading him to view Nazism as a means to escape the prevailing technological mode of being in his time.

Karl Popper, who coined the term *critical rationalism*, which is the bases for how we do science, has no good word for Heidegger.

>Heidegger is, so to speak, the Hegelian of our time, who was also a Nazi, among other things. The worst part is that in Germany and all over the world, for example in South America, France, and Spain, Heidegger is admired and imitated. [...] He usually writes things that one cannot understand at all, and that too, for pages on end! -- Karl Popper

Other notable critics include Karl Jaspers, Theodor W. Adorno (Frankfurt School), Jürgen Habermas (Frankfurt School), Kurt Tucholsky, and numerous others.
Heidegger's thinking and writing have been criticized for dressing up banalities as profound insights, projecting his personal moods onto others, hiding behind an overly complex language, overemphasizing the role of human beings in dertermining the nature of being, lacking any ethical content, circular reasoning and tautologies, being the root for the rise of *relativism* and the rejection of objective turths.

Despite these criticisms, Heidegger remains an influential figure in the history of philosophy, and his ideas continue to be debated and studied in academic circles.
He influenced the works of Herbert Marcuse, Jean-Paul Sartre, Jacques Derrida, Paul Tillich, Michel Foucault and many more.
It would be a mistake to completely dismiss his works because of his character.
But it would also be a mistake to dive into his work without knowing about his actions and connections to the Nazi party.

>How is it that such a brilliant mind was taken in by the Nazis? How could Heidegger be so socially irresponsible? It is the same problem as with Celan: here is a wonderful writer who was a rotten anti-Semite [...] I have shown Heidegger’s brain with a mushroom-like tumor growing out of it to make the point. -- Anselm Kiefer

## Being and Time

In *Being and Time* {% cite heidegger:1927 %} Heidegger is concerned with nature of *Being (Sein)*, which, according to him, should be the central question of philosophy (*existential phenomenology*).
He thinks that previous philosophers have failed to adress this question adequately, confusing the concept of *Being* with particular beings or entities.
He also thinks that Husserl's cause to establish a new kind of foundationalism by the process of bracketing is impossible because bracketing is never complete.
You can not strip the transcendential self bare of the evidence of this that or the other kind of object of knowledge and catch it.
You never catch the relation between subject and object (the intentional state) without the objects.

Heidegger distinguishes between *being (Sein)* and *beings (Seiendem)* and argues that *Being* (universal) is the indispensable reason for *beings* (individual: particular manifestation of the universal).
Furthermore, he uses the term *being-in-the-world (Dasein)* to describe the unique mode of existence of human beings which are beings that *care* (Heidegger's intentionality) about their own existence and embeddedness in a world of meaning and relationships.
Only *Dasein* is capable of questioning *Being (Sein)*, which makes it special.
So do we have the ground of being in the consciousness (Dasein) of our own existence?

>Why is there something rather than nothing? How can I be on this verge of nothingness?

For Descartes, the subject is the seat of experience and is ontologically prior to the world around it. 
Contrary to this, Heidegger's goal is to show that there is no subject distinct from the external world of things. 
Heidegger puts together the separation of the subject and the object by the concept of *Dasein*.

For Heidegger, the *world* is not presented to us as a collection of objects that we look upon from a detached perspective, but as a holistic web of interconnected equipment with which we are inextricably entangled. 
Objects only show up in the context of a background of purposes, concerns, practices and equipmental dealings that is constitutive of our *being-in-the-world*. 
The *subject-object perspective* is thus a derivative, incomplete understanding, which is blinded by its failure to recognise the primacy of "concernful engagement" with things above traditionally-minded epistemological scrutiny.
If we follow Heidegger’s train of thought, we cannot indulge in the kind of skepticism that Descartes presents, since it is predicated on a false distinction between us and the world.
Questioning the authenticity of one's own existence within the world necessitates a profound understanding of that world, which can only be established by that very existence, or in other words, by *Dasein*.

From the title of the book we immediately see Heidegger's emphasis on *time*.
He argues that *Dasein's* experience of time is fundamentally different from the objective, linear conception of time.
*Dasein* experiences time as a unified whole, with past, present, and future interwoven in a complex and dynamic structure.

>Dasein finds ‘itself’ proximally in what it does, uses, expects, avoids -- in those things environmentally ready-to-hand with which it is proximally concerned. -- Martin Heidegger

The concept of *time* is closely linked to the importance of *death* in Heidegger's philosophy.
He analyzes death without contrasting it to immortality, focusing instead on how our awareness of death imbues our lives with meaning.
Human existence is characterized by *Being-Towards-Death (Sein-zum-Tode)*.
Heidegger envisions death not as a fixed point at the end of one's life, but rather as something we continually approach, with the awareness of its inevitability always in the back of our minds.
From the moment we become *Dasein*, which is both embodied and linguistic, we are conscious of time and the approaching certainty of our own demise.

When *Dasein* reflects on the world, it experiences a sense of not being the center of that world, and that other processes and beings have their own concerns and experiences regardless of *Dasein's* presence.
This sensation gives *Dasein* one of its original dispositions towards the world: *thrownness* *(Geworfenheit)*.
It feels as if we are thrown into something already in progress.
Although skepticism may lead us to question whether the world is merely a simulation or a dream, the very act of questioning arises from the experience of thrownness that has shaped our entire lives up to that point.

>Dasein gets dragged along in thrownness; that is to say, as something which has been thrown into the world. [...] It exists as an entity which has to be as it is and as it can be. -- Martin Heidegger

When we consider the future, we undertake projects (*projections*), that is, we consider our possibilities of action.
And from all the things we do, there is only one that we must do and that is to die.
Any possible action we take in time is one step closer towards death, no matter what that action is.
We are aware of this, and it actually changes the shape of all experience.
According to Heidegger, this gives our actions value, because we have to choose which one is most worth doing, considering that anything that you do brings your death one step closer.

>[Death is] the possibility of the impossibility of any existence at all [...] it is the possibility of the impossibility of every way of comporting oneself towards anything. -- Martin Heidegger

As if Kierkegaard is speaking.

Heidegger claims that an original way of being is in the disposition of *care*, meaning that one is open to the world, that one will take an interest in it.
One questions the world and is curious about it.
Our projects matter because each project is a step towards death.

In addition, Heidegger argues that the world is *ready-to-hand* and that we are most human, when we putting our hands on the world, i.e., when we explore the *Handlichkeit* (manipulability) of the world.
We should make our hands dirty and play with the world.

*Dasein* cares about *thrownness* (past) and *projections* (future) but also where it gets its sense of *unfreedom*, which is caused by everbody else.
In some sense one is always a product of the time, place, and culture within which one is born, lives, and dies.
But within this *facticity*, these circumscribed limits, there is freedom -- in fact, the *necessity of choice*. 
In other words, as thrown, we are thrust into a set of circumstances, and freedom lies in choosing to embrace our thrown possibility.
This duality exists at each and every moment of our existence and bears upon our potentiality for being *authentic*. 
We often conform to societal norms, becoming absorbed in politics, TV shows, career expectations, consumption, and idle gossip.
This leads to a *loss of authenticity* and the pursuit of one's own unique projects, as we become part of the collective *They-self (das Man)*.

>The "they", which supplies the answer to the question of the "who" of everyday Dasein, is the "nobody" to whom every Dasein has already surrendered itself -- Martin Heidegger

To be *authentic* is not a moral category.
Rather it is about engaging in activities that define our individuality, which can become overshadowed by the world in a state of *fallenness* (*Verfallens*).
We lose ourselves to the crowd and assimilate with others.
We fall out of ourselves into the crowd.
Again, I can sense Kierkegaard's influence.

According to Heidegger, conforming to what everyone else does merely because it is the norm opposes the concept of exercising one's freedom.
This inauthentic way of living leads to becoming entrenched in the mundane, where curiosity and exploration are replaced by repetitive habits and following trends.

To live *authentically*, Heidegger posits that one must be conscious of death and embrace the freedom that this awareness provides, thus allowing for the pursuit of individuality and personal growth.
For Heidegger, when *Dasein* truly reckons with the reality of death and owns that its fate is sealed by the limitations death imposes, our finitude, the *everyday world* falls away -- others, the objects of concern, everything. 
These are moments of *anxiety*.
Think of what happens to a person the moment they receive a diagnosis of cancer or another terminal disease.
The dread and anxiety experienced in that moment are uniquely their own.
There is nothing anyone can do for them. 
They are completely alone with the knowledge that they could be facing the end (or at least a radically modified existence).
*Dasein* for the most part covers over and flees from this awareness of our being toward death but this reaction is not the only possibility. 
According to Heidegger, it is possible for this truth to somehow be kept in sight.

>When one becomes free for one’s own death, one is liberated from one’s lostness in those possibilities which may accidentally thrust themselves upon one; and one is liberated in such a way that for the first time one can authentically understand and choose among the factical possibilities lying ahead of that possibility which is not to be outstripped [death]. -- Martin Heidegger

For Heidegger, being authentic does not require some exceptional effort or discipline, like meditation. 
Rather, it entails a kind of shift in attention and engagement, a reclaiming of oneself, from the way we typically fall into our everyday ways of being.
It is about how we approach the world in our daily activities. 
*Dasein* inevitably moves between our day-by-day enmeshment with the they (*das Man*) and a seizing upon glimpses of our truer, uniquely individual possibilities for existence.
The challenge is to bring ourselves back from our fallenness in the they (*das Man*) to retrieve ourselves so that we can become our *authentic* selves. 
He does not mean here anything like a moral imperative to do the right thing according to an external law, but rather a clear and focused listening to and heeding of one's unique capabilities and potential.
In doing so *Dasein* authentically understands itself and is able to act in the world accordingly.

As Heidegger sought to think in a pre-scientific manner, he turned to the pre-scientific language of the ancient Greeks.
For instance, "truth" in that era had a different meaning than how we understand the term today. Something that reveals itself to you, or a thing or being that appears to you, was considered true.
Interestingly, this coincides with our expression "a moment of truth," such as when we are compelled to reveal our "true selves".

For Heidegger, *understanding* is a way we project the meaning of our *Dasein* onto objects.
We create objects (that we understand) in our own image.
I make an object for myself; I project my meaning onto it.
Here, we encounter *postmodern* tendencies.
According to Heidegger, we project meaning onto the world, naming things and giving them the meaning we desire, with the way we talk about them being more revealing of ourselves than of the things themselves.
So, what is the quest for truth? 
The quest for truth becomes the pursuit of the unveiling of *Being*.

Heidegger's phenomenology claims to take a step back from the way we ordinarily conceptualize and theorize about reality and begin from immediacy of experience, which has priority for him, and elicit the structure of experience from there.
But in contrast to Husserl, he does not seperate the *I* from *the world*.

## Being and Science

I hope that by now, you have gained an understanding of Heidegger's *existential phenomenological* perspective.
Let's attempt to comprehend science through this lens.

According to Heidegger, science is the theory of reality (Theorie des Wirklichen).
In German, the word for reality, "Wirklichkeit", is derived from "wirken", which means to have an effect or to appear.
What is real is actual (wirklich), or genuinely present (anwesend), regardless of one's standpoint.
That which is real appeals to us by itself, imposing its presence upon us.
It's unsurprising that reality can surprise us, as we may have previously believed things to be different.

To scientifically "capture" reality, we must employ a methodical approach.
Theories should assist us in perceiving reality.
We must envision reality as accurately as possible and derive a theory that allows us to "corner" (stellen) reality.
In doing so, reality falls into our trap.

Science, in contrast to philosophy, must narrow down its focus.
Science does not examine *beings as a whole* or *Being* itself; it only investigates individual beings.
Philosophy, on the other hand, explores the difference between *Being* and *beings*, which Heidegger calls *ontological difference*.

According to Heidegger, scientists must objectify reality in order to extract it.
As a result, everything that is real is transformed into a variety of adversarial aspects. 
What is present is processed into a theory of reality.
Each theory secures a specific domain of reality as its subject matter.
Scientists cannot dissect *Being*; they suffer from an oblivion of it.
And even if they were to combine all the parts and theories, we would still not arrive at a study of *Being*.

In fact, science is so effective precisely because it overlooks *Being*! 
Heidegger goes so far to say that 

>Science does not think. -- Martin Heidegger

Scientists must measure or observe reality using instruments, which serve as traps for reality. 
They rely on tangible aspects of reality and focus on specific elements. 
In this sense, every objectification of what is real involves calculation or counting (mit etwas rechnen).

Undoubtedly, science has provided us with fantastic new tools and insights into reality.
And no one would claim scientists do not think in the sense we understand thinking.
However, Heidegger cautions us that the scientific method requires a certain level of blindness towards the bigger picture and that natural sciences have an exceptional position in our technological society which might be less founded than we think.
In 1951, he already discussed the devastation of the Earth in connection with his understanding of thinking and the role of science.
Scientists may count every tree, measure and observe, but they do not always consider the forest as a whole. 
They often overlook the broader context or *interpretive horizon (Auslegungshorizont)*.
Today, we try to solve climate change using *systems theory*, i.e., a method to act agains our blindness of the bigger picture.
Therefore, I would say Heiddeger had a point.
However, if he is correct *systems theory* shouldn't "work" either.

>Science does not think. This is an offensive statement for ordinary thinking. It does not think because, according to the nature of its procedures and tools, it can never think. The fact that science cannot think is not a deficiency, but an advantage. This alone ensures its ability to engage in a specific subject area according to the mode of research. Let's accept the offensive character of this statement. -- Martin Heidegger

For Heidegger, true understanding of experience must begin with this phenomenological starting point, the immediacy of experience. 

## Meditations on Being

All the fuss about *Being* reminds you maybe about some Eastern philosophy.
It is known that Heidegger was familar with the writings of Zuangzi and many argue that his work is inspired by Eastern thought.
For example, he ask that, instead of investigating reality, we should sit back and let beings, that are present, introduce themselves to us.

Imagine a tree.
Let us assume you are standing in front of the tree.
With all what you are you are standing there and the tree as well.
The tree introduces itself and we introduce ourselves to the tree by standing there.
Again, Heidegger plays with the word *Vorstellung* which means both: introduction as well as imagination.

>Let's pause for a moment, just as when we take a breath before and after a leap. For now, we have indeed jumped, out of the familiar realm of the sciences and even, as will become apparent, of philosophy. And where have we leaped to? Perhaps into an abyss? No! Rather, onto [...] the ground on which we live and die, if we don't deceive ourselves. -- Marin Heidegger

This sounds a lot like the instruction for meditation.
Scientifically this situation remains completely irrelevant.
Standing in front of a tree is probably the most mondane thing ever, right?
Nothing special; who cares?
Important is the non-scientific perspective (also excluding psychology) of the process.
We should not reduce our approch to anything!

>Where is the tree and the human? [...] Where is the introduction that takes place? [...] Is the tree part of our conciousness or is he standing on grassland? -- Marin Heidegger

Even if everything seems to be so trivial for us (as *Dasein*), it is not trivial at all.
Why should we ask such questions?
Isn't it obvious for everyone that we just stand in front of a tree?

>For suddenly, we give away everything as soon as the sciences of physics, physiology, psychology, along with scientific philosophy, explain to us with all the expenditure of their evidence and proofs that we actually do not perceive a tree, but in reality, an emptiness in which sparse electrical charges are scattered here and there, whizzing back and forth at great speed. -- Marin Heidegger

Heidegger argues that, if we are honest, we tend to let go of the concept of a tree and instead adopt the scientific view of electrical charges, even though science cannot examine its own essence and is blind to *Being*.
We are inclined to believe we understand what is happening, and if we do not know, we trust that science will eventually provide the answers.
Heidegger claims that this assumption is incorrect.
Occasionally, it would be beneficial to simply allow *beings* to exist as they are.
Rather than conceptualizing and relying on reality, we should experience it.
We should seek out *beings* that reveal themselves to us.
*Being* is more expansive than we think, if we give it space.

## Spaces of (Im)Possibilities

Technology is often viewed as having both positive and negative effects, with its impact largely dependent on who controls it.
Many people see technology as a mere tool, adhering to the notion that guns don't kill people, but people do.
In their white paper {% cite eu:2020 %} the *European Comission* states:

>Like any technology, AI brings opportunities and risks. -- {% cite eu:2020 %}

This quote presumes that humans have full control over technology.
Janina Loh refers to this as the *Neutrality Thesis of Technology* {% cite loh:2019 %}.
Numerous other philosophers challenge this perspective as well, arguing that technology can embody inherent values that may not be immediately apparent.

For example, in the 1920s, architect Robert Moses designed overpasses in New York that were too low for buses to pass beneath.
As buses primarily served low-income people of color, this design choice restricted their access to areas that white residents wanted to keep segregated.
Architecture is a form of regulation and control.
If a building has no entrance for wheelchairs, its design denies access for certain people;
By definition this is a form of discrimination -- one that our society no longer "likes" since there is no more justification.
Not all forms of discriminations are seen as unjustfied today.
For example, the missing voting right for children is a form of discrimination that we accept.

Another prime illustration of this principle can be seen in digital spaces.
These environments are imbued with values, determined by the control and regulation of permissible actions.
Digital spaces, or *cyberspace*, as it's often termed, have never been free from regulation, despite common misconceptions.
Rather than being shaped by traditional architectural forms, digital spaces are regulated by *source code*.
The design of websites or applications, dictated by this code, significantly influences the behavior of their users.
In other words, the very structure of these platforms functions as a form of regulation, guiding user interactions and activities.

>Codes constitute cyberspaces; spaces enable and disable individuals and groups. The selections about code are therefore in part a selection about who, what, and, most important, what ways of life will be enabled and disabled. -- {% cite lessig:2006 %}

Web pioneers often look back at the "good old days".
Their (mine included) romantic view of the beginnings of the internet is that everything was possible.
It was easy to build a website.
A little bit of HTML, which you could hack into a simple text editor, and you are done.
However, without regulations coming from companies or institutions, social norms and code regulated the web instead.
And since information technolgy was rather new, a few individual coders had a lot to "say".
Today, these individuals got replaced by a view companies.

Similar to Lessig, Don Ihde discusses in *Technology and the Lifeworld*, technology as an expandasion or limit of possibilities.
Technology goes beyond mere functionality or direct control.
{% cite rosengruen:2022 %} argues that Western societies are currenty shifting from *rule of law* to *rule of code*:

>[...] source code is about to become the main regulator of individual and institutional behavior that regulates all other regulators including law -- {% cite rosengruen:2022 %}

Rosengrün's argument is not centered on the premise that regulation is inherently detrimental, or that large companies harbor malicious intentions.
Instead, he acknowledges the necessity, and indeed the inevitability, of regulation.
Moreover, he observes that the alignment of corporate regulatory practices with their profit-making objectives is to be expected.
He forcefully advocates for the *rule of law* as an indispensable prerequisite for a democratic society.
Consequently, any attempt to replace it, could precipitate the dissolution of such a society.
For this reason, Rosengrün asserts that code must be not only open but also subject to regulation.
He underscores the potential dangers of allowing code to regulate law.
For instance, when machine learning is used in the policy-making process, it is unrealistic to expect that law will maintain supremacy over code.

Sam Altman, the CEO of OpenAI, recently proposed a licensing-based regulation during a US Senate hearing. 
This proposal is concerning as it would potentially undermine the open-source community, achieving exactly the opposite of the intended effect! 
While licensing could introduce some safety measures, ultimately it would protect large corporations and obscure the inner workings of the technology that regulate regulations.

The notion of *regulation by code* is familiar to us programmers, as we understand that even the selection of a programming language can either expand or limit our possibilities.
For instance, if one's objective is to engage in machine learning, the programming language Scheme wouldn't be the optimal choice.
Furthermore, the popularity of specific languages can influence their usage.
It's important to note, however, that this degree of openness isn't solely controlled by a single programmer. 
Instead, it's shaped by the broader social and technological systems that surround us.
As a significant amount of control lies with large corporations, they inherently influence the horizon of the possible.

Technology not only provides a framework for action, but also actively shapes our environment.
It constructs particular subjects and defines our identities, delineating not only who we are but also who we can potentially become.

Another stark illustration of technology's pervasive (positive) influence can be found in modern birth control methods and in the strategies we deploy to shape our identities, such as curating online profiles to garner peer validation.
It's widely acknowledged that social media can inadvertently amplify negative aspects of human behavior, underscoring the necessity to incorporate media into our understanding of this phenomenon.
Further instances of technology's impact manifest in the realm of beauty and medical advancements. 
Technologies related to beauty and cosmetic surgery exert pressure to conform to evolving standards of beauty, standards that were unattainable just six decades ago.
Similarly, assistive tools have profoundly enhanced the agency of individuals with disabilities, and advancements in medical transitions have carved out new avenues for expressing and experiencing transgender identities.

Adopting a phenomenological viewpoint, it becomes clear that the subject and the technology it employs cannot be disentangled from each other.
We are back at the concept of *Dasein*.

## The Essence of Technology

I think, Heidegger goes a bit further.
He thinks of technology as a *mode of seeing* where every "seeing" has its necessary blind spots.

He expands on the example of the hammer to illustrate his thoughts about technology:
when using a hammer (a form of technology), you don't even think about the tool itself; instead, you focus on the nail.
The hammer recedes from your awareness, and you concentrate on the task *at hand*.

Or imagine a keyboard.
As you become proficient at typing, your primary experience involves the words appearing on the screen rather than the act of pressing the keys.

Technology vanishes from our *world*.
Only if it breaks or fails to perform as expected it introduces itself again.
Otherwise, it simply serves as a medium through which we experience our world.
For Heidegger, technology withdraws from our attention and becomes transparent.
We don't experience the technology itself, but rather the world through it.
Consequently, technology introduces its own *mode of seeing*.

When you drive a car proficiently, you develop a sense of the vehicle's size and whether it can fit through a gap.
Similarly, wearing nail extensions frequently can cause you to forget about them, even though they change how you interact with objects.

Part of the allure of technology is found in the diverse opportunities it provides for unique forms of embodiment.
Philosopher Bruno Latour posits that when technology becomes sufficiently transparent, it becomes an intrinsic part of our self-perception and our experience of the world, thereby giving rise to a new, compound entity.
Evidence supporting this viewpoint is substantial (citation needed).

Our enjoyment of driving stems from the fact that we transform into the driver, a being with speed and power surpassing any land creature.
Likewise, our delight in operating digital devices arises from the sensation of becoming a being that can converse with someone on the opposite side of the globe at a speed comparable to a lightning strike.

Similarly, a person wielding a hammer essentially evolves into a new subject, equipped with its own unique way of perceiving the world. This newly formed subject, as a result of the technology's influence, experiences the world from a distinct perspective and possesses an individualized subjectivity.

If this assesement is correct and a new piece of technolgy comes along, we can and should ask:

>What kind of Dasein will this turn us into?

Heidegger takes this exploration another step further when discussing not merely specific technologies, but the very *essence of technology* itself.
In his essay *The Question Concerning Technology* {% cite heidegger:1954 %}, he challenges the prevailing views of technology as merely an instrument (a means to an end) and as a product of human activity.
He posits that while we perceive ourselves as creators of technology, in reality, technology originates from elsewhere.
He also presents technology as potentially highly dangerous.

Heidegger asserts that perceiving certain types of technology as instruments or tools is itself a technological *mode of understanding*.
He argues that there is no such thing as a purely instrumental object.
Instruments only manifest to those who interpret the world instrumentally.
In this view, technology is a way of comprehending the world that reduces it to raw material, encouraging us to see the world solely as a realm for our intervention.

In ancient times, the Greeks used the term "techne" to denote what we now call technology.
However, their concept of "techne" embraced both art and technique, something akin to our modern notion of "craft".
For the Greeks, "techne" implied the act of bringing something into existence.
It was not about an engineer constructing an object; rather, the craftsman's role was to assist in the emergence of something, so to say helping the material to take on a new form.

A craftsman would never consider themselves as the absolute origin of the reality of what they were creating.
In Heidegger's view, they would perceive themselves as facilitating the process of something coming into existence.

In stark contrast, modern technology is not seen as aiding something to emerge.
Instead, as Heidegger posits, it forces things into existence.
We have come to perceive ourselves as the source of what we create, believing that we are the origin of everything.
This is a highly specific way of interpreting the world, one that seemingly grants us the power to control everything around us.
Therefore, technology unveils the world in a particular way -- it reveals the world as raw material and conceals *Being*.

Given that technology represents a *mode of understanding* the world, and acknowledging that our understanding of the world is not entirely self-determined, this perspective seems to envelop us; it is greater than us as individuals.
Heidegger sees the "highest danger" of technology as its potential to erode our ability to interpret reality profoundly.

The moment we start viewing ourselves as manipulable raw materials, we cease to see ourselves as the ultimate source from which fresh interpretations of the world can spring.
Furthermore, technology can trap us in a specific worldview because as soon as we attempt to devise a new way of interpreting the world, we find ourselves trying to exert power over our inherent *will to power*.
In essence, we attempt to control our very propensity for control.
Every effort to overcome this *will to power* only serves to reinforce it.
Consequently, every attempt to break free from this technological mindset only propels us back into it.

Heidegger suggests a balance: to release, to "let it be".
That is, to both say "yes" to technology, accepting its presence, while simultaneously saying "no" in order to maintain a certain distance.
This balance keeps us open, prepared, ready for the emergence of new interpretations of the world.
How they can come to us remains a mystery.

>Philosophy will not be able to bring about a direct change of the present state of the world. This is true not only of philosophy but of all merely human meditations and endeavors. Only a god can still save us. I think the only possibility of salvation left to us is to prepare readiness, through thinking and poetry, for the appearance of the god or for the absence of the god during the decline; so that we do not, simply put, die meaningless deaths, but that when we decline, we decline in the face of the absent god. – Heidegger, Spiegel inverview in 1966

## Language Games

When we examine our languages, it becomes apparent that some words are rooted in experience. 
Although the relationship, as previously mentioned, is arbitrary, it nonetheless exists. 
For instance, the word "dog" is based on the experience of a specific type of being. 
However, according to Ludwig Wittgenstein, this explanation lacks precision. 
In fact, the word "dog" is rooted in numerous experienced situations, and its meaning depends on the *language game* we are currently engaged in.

Take, for instance, the term "sharp" as applied to a knife. 
In a primary school context, the knife may be deemed "sharp," whereas in a kitchen context, the same knife could be seen as "not sharp". 
The knife's sharpness is contingent on the practice for which it is used.

Similarly, as a chemist, one might describe a glass as "full," while a physicist, recognizing that most of the space between atoms is empty, might argue that the glass is "not full". 
Hence, words are anchored in multiple *language games*, that is, the activities and practices in which they are employed. 
They are not metaphysically tethered to a specific object.

One might assume that in chess, the "knight" corresponds to the distinct horse-like object. 
However, the knight is a unique kind of piece that alone can leap over other pieces. 
Its movement is singular: one step straight and one step diagonally. 
A word is akin to a chess piece in this respect; 
its meaning is in what it can do, not the object it represents. 
Moving a knight to C3 does not depend on the horse-like object itself. 
The move can be executed regardless of the piece's form, whether with a penny or digitally on a computer. 
In chess, the "knight" derives its meaning from what one can do with it. 
Uttering a word is similar to making a move in the *language game* you are participating in. 
The meaning of the word resides in the activities and interactions within a given environment that are linked to the use of that word, rather than the corresponding object.

Wittgenstein believed that a significant amount of confusion arises when we mix up *language games*, especially in metaphysics. 
He argued that many metaphysical questions stem from a misuse of language.

For instance, a friend of mine is an artist, and she has crafted impeccably convincing replicas of everyday objects. 
During one of my visits, I asked her: "Is this apple real?" She replied, "Yes, you can eat it." 
We were playing the same *language game*, and she understood my question. 
She was aware that I might have mistaken the apple for one of her replicas. 
She even recognized that my question implied my desire to eat the apple. 
However, if I were to pose the same question to a fruit vendor at a market: "Is this apple real?" he would probably answer, "Yes," albeit with some confusion. 
My question doesn't make sense in the *language game* he's playing; we're in different games, and in his game, my question is meaningless.

If we engage in a mathematical *language game*, the seemingly reasonable question "is something real" (in an existential sense) might to be meaningless. 
Such a question makes perfect sense when suspicion of imitation arises, but in math, querying whether numbers "exist" might simply be a misuse of language. 

According to Wittgenstein, philosophers have attempted to "fix" this misunderstanding by constructing a system, essentially engaging in metaphysics. 
Yet, ironically, it is language that can lead us astray while remaining highly transparent.

Nonetheless, this do not imply that words are not grounded in reality. 
Quite the contrary, they are deeply rooted, albeit in a much more intricate manner! 
For instance, the relational context of words within a text hints at the *language game* in play. 
Only once we understand the game, we can be confident about a word's intended meaning.

Why is this significant in conversations surrounding *artificial intelligence*? 
Indeed, if an AI, such as ChatGPT, aims to be valuable, it must at least mimic an understanding of *language games*. 
Interestingly, well-constructed prompts frequently serve to illuminate the language game that the chatbot is participating in. 
Moreover, meaning can only emerge when words (or other symbols) are grounded in experience. 
Hence, it is tempting to assert that without experience, *strong AI* is unattainable. 
Finally, it could be argued that the use of the term "intelligence" in the context of AI might constitute a misuse of language.
 
## The Chinese Room Argument

John Searle, in his *Chinese Room argument*, asserts that symbolic AI will never achieve strong AI status, meaning it will remain a simulation of intelligence rather than truly embodying it.

Here's how the argument unfolds: Imagine a person, let's call him Bob.
Bob is confined within a room that houses a book of instructions written in Chinese. 
Bob neither understands nor speaks Chinese, and his only means of communication with the outside world is a small slot through which he can pass written text. 
Outside the room, there's a person who is fluent in Chinese. 
This person writes a message on a piece of paper and slides it through the slot into the room. Bob, upon receiving the text, consults the instruction book to find the appropriate response, writes it down, and sends it back out through the slot.

From the perspective of the person outside the room, it appears as if Bob can comprehend and communicate in Chinese, despite Bob's ignorance of the language. 
This situation is similar to the operation of a computer: it receives input and employs specific rules to produce an output.

Searle concludes that, similar to Bob, computers do not understand what they compute.

However, I believe this argument lacks substantial strength as it hinges on the presumption that there is a single atomic entity (Bob) that "understands" Chinese. 
Consider a single neuron in my brain, for instance. 
This neuron doesn't understand English on its own. 
Therefore, following Searle's logic, one could conclude that I don't understand English. 
But the reality is that my entire system comprehends English. Following this reasoning, one could argue that the entire room, in Searle's scenario, comprehends Chinese.

Presumably, what Searle is trying to convey is that the Chinese language, in this scenario, isn't *grounded* in experience. 
Being able to use words and construct grammatically correct sentences doesn't necessarily equate to understanding the content, as there's no connection to personally experienced situations. 
A more compelling argument on this topic has been made by Dreyfus.

## How to Become an Expert?

In {% cite dreyfus:1986 %}, Dreyfus provides a compelling critique of *symbolic AI*, examining the chasm that exists between *expert systems* and *human experts*.
While reading the following argument, think about *sub-symbolic systems* such as *artificial neural networks*.
Do they suffer from the same deficiencies?

An expert system is a computer program designed to emulate the decision-making capabilities of a human expert using if-then rules instead of conventional procedural code. 
The presumption underlying this concept is that experts adhere to precise, unambiguous rules, distinguishing them from non-experts. The theory suggests that it's possible to codify this expertise into symbolic rules and then incorporate them into a machine. 
Therefore, the idea is that any entity, human or machine, with access to these rules could potentially achieve expertise.

Dreyfus dissects this reasoning through a Heideggerian lens. 
We can scrutinize this argument more closely by posing some simple but probing questions:

1. Does an expert system genuinely replicate the practices of human experts?
2. Are you an expert in any field?
3. How did you become an expert?

According to Dreyfus, humans acquire expertise through various stages. 
Let's consider learning to drive as an example. 
A novice driver learns to gauge her speed by referencing the speedometer. 
She is given rules such as "shift to second gear when the speedometer reads ten miles per hour". 
For a beginner, driving can be exceptionally slow-going, as one must remember all the rules and features.

As the practice continues, the beginner becomes aware of various aspects of the situation. 
After exposure to numerous examples, the student learns to recognize these new elements. 
Instructions can now refer to these newly understood situational aspects, as well as the objectively defined, non-situational features recognizable to the novice.

The advanced beginner driver, utilizing both situational (engine sounds) and non-situational (speed) elements in their gear-shifting rules, learns the maxim: shift up when the engine sounds like it is racing and down when it sounds like it's straining. 
She learns to observe the demeanor, as well as the position and velocity of pedestrians or other drivers. 
For instance, she can differentiate the behavior of a distracted or inebriated driver from that of an impatient but alert one. Engine sounds and behavior styles are difficult to encapsulate in words, making choice examples vital for learning such distinctions.

Upon reaching the stage of competence, the student learns to differentiate between important and unimportant elements of the situation. 
With growing experience, the number of potentially relevant elements that the learner can recognize becomes overwhelming. 
This complexity might lead the student to question how anyone could master the skill. 
To manage this overload and attain competence, learners, through instruction or experience, devise a plan or select a perspective. 
This perspective then determines which elements of the situation should be treated as significant and which ones can be disregarded. 
By focusing attention on only a few of the many possible relevant features and aspects, such a perspective simplifies decision-making.

The competent performer thus seeks new rules and reasoning procedures to determine a plan or perspective. 
However, these rules aren't as straightforward as the rules and maxims provided to beginners. 
There are simply too many situations, each differing subtly from the others. 
In fact, there are more situations than can be named or precisely defined, so no one can prepare a comprehensive list of actions for every potential situation. 
Thus, coping with this complexity becomes challenging, rather than exhausting. 
At this stage, if the learned rules fail, the performer might feel remorse for their mistake rather than rationalizing that they hadn't been given adequate rules.

A competent driver, considering factors like speed, road conditions, and time pressure, may decide she is going too fast. 
She then has to decide whether to ease off the accelerator, remove her foot entirely, or step on the brake, and determine precisely when to do so. 
The driver may feel relieved if she navigates the curve without eliciting honks from other drivers, and alarmed if she begins to skid. Successful plans trigger feelings of euphoria, while mistakes create a sinking feeling.

As the competent performer becomes increasingly **emotionally** invested in their task, it becomes harder to maintain the detached rule-following stance of the beginner. 
While it may seem that this **emotional** involvement could hinder objective rule-testing and thus impede further skill development, the opposite appears to be true. 
If the learner's experiences are **emotionally** charged, the resulting positive and negative experiences will reinforce successful responses and inhibit unsuccessful ones. 
The performer's theory of the skill, represented by rules and principles, will gradually be replaced by **situational discriminations** paired with associated responses. 
Proficiency seems to develop if, and only if, experience is assimilated in this atheoretical manner and intuitive behavior replaces reasoned responses. 
Action becomes easier and less stressful as the learner simply **sees** what needs to be achieved (rather than what actions to take to achieve it), rather than deciding through a calculative procedure which of several possible alternatives should be chosen.

The proficient driver, approaching a curve on a rainy day, may feel in her gut that she's going dangerously fast. 
She must then decide whether to apply the brakes or merely to reduce pressure on the accelerator by a certain amount. 
The proficient performer, immersed in her skilled activity, can discern what needs to be done, but must decide how to do it.

The expert, thanks to a vast repertoire of situational discriminations, not only **sees** what needs to be achieved; she intuitively understands how to accomplish her goal. 
This capacity for subtle and refined discernment sets the expert apart from the proficient performer. 
Through ample experience in various situations, the expert performer's brain gradually subdivides this class of situations into subclasses, each associated with the same response. 
This allows for the immediate intuitive situational response that characterizes expertise. 
Driving likely demands the ability to discern a similar number of typical situations. The expert driver not only senses when it's necessary to slow down on an off-ramp; she naturally performs the appropriate action.

A beginner relies on rules and facts for decision-making, much like a programmed computer. 
With talent and extensive, engaged experience, the beginner evolves into an expert who intuitively knows what to do without resorting to rules. 
Normally, an expert does not deliberate. She doesn't solve problems. 
She doesn't even consciously think. She simply does what usually works, and of course, it usually does.

This description of skill acquisition illuminates why experts often struggle to articulate the rules they're following: they're not abiding by any predefined rules! 
Instead, they're discerning thousands of unique cases. 
This also explains why expert systems never quite match human experts. 
If you ask an expert to describe the rules she's using, you're essentially compelling her to regress to the level of a beginner and recite the rules she learned at the outset. 
Thus, instead of applying rules she has long forgotten, as knowledge engineers might assume, the expert is forced to recall rules she no longer utilizes. 
If these rules are programmed into a computer, the computer's speed, accuracy, and ability to store and access millions of facts could surpass a human beginner using the same rules. 
However, such systems can only achieve competence at best.
No number of rules and facts can emulate the knowledge an expert acquires when she has accumulated the experiences and outcomes of tens of thousands of situations.

Hubert Dreyfus, in his critique, directly targets the fundamental assumption underpinning symbolic AI, which he labels "the psychological assumption". 
He defines it as follows:

>The mind can be viewed as a device operating on bits of information according to formal rules.

Dreyfus challenges this assertion by illustrating that human intelligence and expertise primarily depend on unconscious instincts rather than conscious symbolic manipulations. 
Experts swiftly resolve problems by leveraging their intuitions, not through methodical trial-and-error searches.

However, Dreyfus' insights extend beyond this academic revelation. 
He cautions, in a vein similar to Heidegger, that treating rationality as rule-governed behavior presents substantial risks to society. 
He posits that the calculative conception of reason underpins a broader trend towards calculative rationality in our culture, a trend laden with potential peril. 
The escalating bureaucratic nature of society -- a society abstracted and regulated by rules -- heightens the threat that future skill and expertise may erode due to an excessive reliance on *calculative rationality*.

## Conclusion

I have offered you my understanding of some of Heidegger’s philosophy. 
What are my personal thoughts on it? Well, I believe that he undeniably provides some insightful perspectives. 
His portrayal of the human condition as *Dasein*, thrown into a world in progress, resonates with my personal experience. 
It counters the illusion that we possess complete control over everything.

Even if our *Dasein* cannot be objectively measured, it remains an undeniable presence -- the subjective experience of our world matters, regardless of its "rationality". 
A person suffering from paranoia may be irrational, but their experiences are real and impactful. 
Moreover, our existence is neither strictly mental nor physical; 
instead, it extends through time and space and is not so easily separable from the world. 
If I write a note on a piece of paper, does this note become part of my being? 
If someone steals my mobile device containing all my essential data, feeling as though my identity as my understanding of "who I am" and "how I see the world" has been robbed, might be a reasonable response.
This partially explains why many individuals enjoy driving fast cars, despite their detrimental impact on the environment. 
The car and driver together form more than a mere "thing";
they create a composite being.

Heidegger's emphasis on the *ordinary life* may seem trivial, yet, how frequently do we feel lost, overwhelmed, and alienated in the world, even though we are experts in navigating this chaos? 
Often, we forget that we are temporal beings in a world where we are deeply rooted.
This fact becomes crystal clear when considering all the small habits our bodies carry out so effortlessly that we hardly give them a thought.
All the smells we can perceive, all the various tactile experiences we can receive and give, and the ways in which we can walk, sit, run, and dance. 
Sometimes, when one feels lost in the digital spectacle, returning to the simplicity of the ordinary can offer a reassuring sense of home.

Heidegger's intense concerns regarding technology have significantly influenced many thinkers.
This perspective challenges the mainstream belief that technology is inherently neutral.
It is tied to the notion that we have far less control over our *being-towards-death* than we typically presume.

Few would dispute that technology has transformed our self-perception. 
Sigmund Freud already famously introduced three humiliations:

+ Earth is no longer the center of the universe.
+ Humans are not the apex of creation but part of an evolutionary biological process.
+ The human "ego" is not fully in control but is governed by unconscious processes.

To this, I would append Nietzsche's declaration that:

+ God is dead, implying that normative values are constructed and lack any metaphysical "essence".

We might further speculate two additional humiliations:

+ The human "ego" or "self" is an illusion.
+ Machines are more intelligent than us.

Despite these considerations, it seems that we still resist the notion that technology could or have already changed us in undesirable ways, even if we take all the precautions to avoid negative outcomes. 
Isn't that so? 
Even though the European Comission upholds the *Neutrality Thesis of Technology*, considerable distrust and resistance exist, often funneled into dystopian futures envisioned by the *cyberpunk* genre. 
Therefore, I argue, we can no longer envision having a choice in which technological path we take!
We lack alternatives; we lack the capacity to dream of a different future.
Heidegger may indeed be right when he describes our *technological mode of being*, wherein we perceive the world and its inhabitants as tools -- entities we can dominate and control at will.
His student Hannah Arendt later emphasised her fear of our inability to think (in an Heidegerian sense) and consequently to steer historical developments.

>It could be that we, who are earth-bound creatures and have begun to act as though we were dwellers of the universe, will forever be unable to understand, that is, to think and speak about the things which nevertheless we are able to do. -- Hannah Arendt

Heidegger was onto something significant. 
After all, his critics from the Frankfurt School, such as T. Adorno, partially concurred when they discussed the concept of *instrumental rationality*, which often leads to irrational outcomes. 
However, in stark contrast, they argue that we are not rational enough. 
In other words, decisions that are instrumentally rational may cumulatively produce irrational results -- a viewpoint compatible with *systems theory*. 

For instance, it may be entirely instrumentally rational to use cheap oil to bolster my company's profits, but this contributes to the death of people -— an outcome that is irrational. 

If we aim to combat climate change, we likely need to set aside Heidegger's philosophy with respect to technology.
Indeed, Heidegger offers no concrete solution and leaves us in a state of uncertainty, waiting for a God who might rescue us. 
Perhaps, from his perspective, climate change is the God we await.

Our response to the increasing frequency of anticipated climate disasters is primarily technological.
It's not God, but artificial intelligence and other technologies that we hope will save us.
And I must believe in this, because I can't conceive of any alternative that wouldn't result in unprecedented loss of life. 
However, this widely-held view serves to further empower the masters of technology.

>The everyday demonstration of the power of our technology grants the priests of the church, who make all of this possible, almost limitless authority and credibility. -- {% cite weizenbaum:2001 %}

As noted in {% cite rosengruen:2021 %}, a viewpoint often propagated, referred to as the *transhumanist* view, suggests that each individual has the choice of whether to utilize technology or not. 
However, similar to the *Neutrality Thesis of Technology*, this *freedom of choice* only makes sense when viewing technology from a strictly *STEM* (science, technology, engineering, mathematics) perspective.

As McLuhan points out in {% cite mcluhan:1964 %}, the true substance of a piece of technology isn't defined by what it seems to convey, but rather how it interacts with society as a whole. 
He asserts that advancements in *communication technology* are the key drivers of societal change, rather than the content being communicated.

>The medium is the message -- {% cite mcluhan:1964 %}

The promise that technology will enhance our lives paradoxically echoes the belief in metaphysical concepts that self-proclaimed rationalists often disdain; 
it resembles a new form of religion. 
Furthermore, when we examine the material reality, we fall short in providing even low-tech technology to those who need it most. 
We don't even have to consider ambitious endeavors such as colonizing Mars or turning space travel into a commercial venture.

For instance, many disabled individuals do not face a struggle in identifying suitable assistive devices, but rather in acquiring them. 
These tools may not be affordable, or their production may have been discontinued due to insufficient demand. 
Often, these individuals grapple with obtaining basic equipment such as walking sticks, forceps, wheelchairs, and other low-tech tools. 
Believing that highly sophisticated robots can grant them more autonomy, when one examines the situation systematically, requires a considerable leap of faith.

Of course, there are also wonderful examples of scalable technologies available to many people. 
However, the crux of the matter is that the adoption of new technologies is neither avoidable nor optional; 
quite the contrary, it is often inevitable and if decisions are made by a view "priests" we can no longer call our society democratic.
Furthermore, while these "priests" may not harbor any malevolent intentions, their motivations most certainly do not completely align with the interests of the general public and according to {% cite rosengruen:2021 %} they assume to have many answers which are debated to this day and consequently quite sucessfully plant a new concept of being human in the public discourse -- a naive techno-futurism that lost awe to the human condition (*Dasein*).
Weizenbaum aptly identifies the devastating effects that such transhumanism unfolds in public:

>Firstly, to impress them with the depth of the task that the AI is dealing with. Secondly, to suggest the progress that has already been made. And thirdly -- and this is particularly important -- to propagate a certain image of humans as completely self-evident in society. {% cite weizenbaum:2001 %}

While I acknowledge the robustness of Heidegger's critique of technology, I do not subscribe to the distinction between what he terms as the benevolent old technology and the malevolent new kind, such as differentiating between a windmill that passively harnesses wind and a turbine that forces a river into a specific course. 
I view this distinction as arbitrary.

Moreover, categorizing perspectives as either *instrumental* or *non-instrumental* seems to introduce an unnecessary binary. 
This can lead us into the nostalgic trap of perceiving traditional, conservative, and thus hierarchical ways of life as more humane. 
Consequently, the leap to slogans such as make our country great again, advocating a return to some supposedly better past, becomes less significant. 
However, this should not deter us from critiquing technology.

Let's consider an analogy: the concept of *free* and *instrumental play*, as introduced by Wolfgang Iser in {% cite iser:1993 %}. 
Iser labels play in games with specific goals as *instrumental play*.
At the other end of the spectrum is *free play*, a form of play devoid of definite endpoints, perpetuating the continuity of play.

*Instrumental play* is a goal-oriented approach that values efficiency, expertise, and optimized strategies as components of play. 
For instance, if children engage with their surroundings, such as running around, this can be considered *free play*.
However, when they introduce goals like catching one another, the play tends more towards *instrumental play*.
The aim of playing isn't solely to reach the end but to discover the most effective way of getting there.

However, there's no such thing as pure *free* or pure *instrumental* play.
Playing chess to win money leans heavily towards *instrumental* play, while playing with a ball purely to enjoy the physical experience is predominantly *free* play. Furthermore, there's no inherently "good" or "bad" style of play. 
In fact, the issue sometimes arises when we deem *instrumental play* to be the correct, or "good," way to play.

But here's the crucial point: *instrumental play* isn't the opposite of *free play*; 
instead, there exists a tension between the two. 
One can derive deep enjoyment from improving a skill, achieving goals, or uncovering solutions to complex problems. 
Similarly, we can perceive the Earth and its inhabitants instrumentally (as a means to an end) or appreciate them for what they are (as ends in themselves).

Therefore, I favor the Kantian notion extended to all beings as well as things -— never viewing anything merely as a mean to an end, but always as an end in itself. 
Furthermore, just as *instrumental play* can enrich *free play*, such as by enabling other forms of *free play* via the acquisition of new skills and habits, I also believe that the scientific perspective, which necessitates objectifying entities and often neglects the broader context, can still enhance our appreciation of the whole.

>I have a friend, who is an artist. And he is sometimes taken a view which I don't agree with very well. He hold up a flower and says: "Look how beautiful it is." And I agree. And he says: "I as an artist can see how beautiful that is but you as a scientist take this all apart and it becomes a doll thing." And I think he is kind of nutty. First of all the beauty that he sees is available to other people and to me too. I may not be quite as refined aesthetically as he is but I can appreciate the beauty of the flower. At the same time, I see much more about the flower than he sees. I can image cells in it; the complicated actions inside which also have a beauty. I mean it is not just beauty at this dimension of one centimetre, there is also beauty at a smaller dimensions; the inner structure also the processes; the fact that the colors and the flowers evolved in order to attract insects to pollinate it is interesting. It means that insects can see the color. It adds a question: does this aesthetic sense also exist in lower forms? Why is it aesthetic? All kinds of interesting questions which the science knowledge only adds to the excitement and mystery in the aura of a flower; it only adds. I don't understand how it subtracts. -- Richard Feynman

Regarding Heidegger's concept of *Being*, I have also reservations. 
It appears to me that he has introduced yet another fundamental entity, much like Plato did with his *world of ideas*. 
It's peculiar that by substituting *Being* with *God*, some of Heidegger's works take on a biblical mysticism.

Probably unintended, Heidegger seems like a preacher of *Being*, that is, guiding us to the "right" and *authentic* path of life by focusing on *Being*. 
Moreover, whenever the term *authentic* is used, I tend to be suspicious. 
It's the most effective pitch of any mystical guru: be *authentic*; 
find yourself. 
Nobody wants to be inauthentic, a sheep in the herd; 
everyone wants to feel special and be part of an extraordinary group.

It's also unsurprising that this *esoteric mysticism* can swiftly lead to *fascism* because one becomes an entity elevated above the herd. 
After all, there's a desire to "correct" others who are on the "wrong" path, and if they are not "fixable," more drastic measures might be employed. 
While there is no causal link, and many *esoteric* people strongly oppose *fascism*, there are tendencies, and we can observe a correlation between *esotericism* (and techno/science scepticism) and *fascism*, at least in Germany during the time of the pandemic.

Heidegger likely wouldn't view the philosophical "advice" discernible in his writings as something to be cultivated in others or even consciously in ourselves. 
Such a contrived promotion of these concepts would be too subjectivist, *instrumental*, or even moralistic, which is not Heidegger’s intent. 
Yet, how people interpret and use his texts is up to them.

Depending on its definition, being *authentic* is nearly always either impossible or so loosely defined that it becomes meaningless. 
Heidegger's definition tends to lean towards the latter. 
It also seems slightly contradictory since we are already *thrown* into a world in progress; 
how can we be anything other than what we already are, including how we attend to and engage with our existence? 
Indeed, we can stand up for our beliefs, be responsible, and honest with ourselves and others, but if we strip away everything we've picked up from others, we're probably left with nothing.

## References

{% bibliography --cited_in_order %}