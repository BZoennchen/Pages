---
layout: post
title: "Revisiting the Case for Heideggerian AI"
tags: 
  - AI
comments: true
---

In our article *From intelligence to autopoiesis: rethinking artificial intelligence through systems theory* {% cite zoennchen:2025 %}, we argued for a shift in vocabulary when communicating about AI because our dated vocabulary discloses a better understanding of how AI systems function as computational systems that do not construct meaning themselves but enable its construction in pyschic (minds) and social systems.
The problem is that algorithms constructed by training data no longer work like tools nor are they living beings which we understand as *organisationally autopoietic* entities.
Using the vocabulary of systems theory, we argue that while classical software is *strictly* *coupled* with minds via formal languages, large language models (LLMs) are *loosly coupled* to minds (thoughts) and to social systems (communication) via natural language.
The difference appears if we look how the operations of LLMs come about:

>While the output of an ANN is theoretically reproducible for a given input (factoring in pseudorandomness), the genesis of its computational operations remains opaque due to the contingency of its training data. 
>This data, shaped by the selections of a contingent society, is neither necessary nor impossible---it is simply improbable in its current form.
>Its structure does not reflect an 'objective' reality historically evolved observational selections. 
>As Luhmann argues, data---like sense---is not discovered but constructed by systems that differentiate themselves from their environment.
>[...T]he operations of ANNs form a new type of structural coupling with social systems---not through direct interaction, but by recognizing patterns in digitized communication within their training data.
>Unlike classical machines, which are strictly coupled to psychic and social systems via formal languages, ANNs introduce a second, looser coupling through the 'digestion' of training data derived from memorized communcation of social systems. 
>In this way, they parasitically 'absorb' social contingencies. 
>In other words, they integrate perturbations on their own terms, that is, computationally. -- {% cite zoennchen:2025 %}

*Contingency* is our replacement to the impossibility of *free will* and LLMs (as far as we know it) do not process it internally but parasitically absorb a new configuration of it.
Thus, their impact on minds and social systems does not arise from intrinsic sense-making but from their influence on communication.
Therfore, we argue that LLMs represent a noval, hybrid form of cognition (using Luhmann's definition) and interaction, whose sociatal and epistemic implications warrent furhter investigations.

## The Grounding Problem

In the paper we also raise concerns about arguments made by {% cite bender:2020 shanahan:2024 %} which may rely too heavily on a **grounding in the world** to assign *understanding* to a system.
These authors argue that LLMs are only trained with form (a purely linguistic reality) and not its meaning thus they can not understand anything.
They do not solve the so-called *grounding problem*: How can a system of symbols (like words in a language, or tokens in a computer program) get their *meaning* in a way that is connected to the real world, rather than just to other symbols?
One way to attack this argument is to generalize symbols to 

