---
layout: post
title: "Re-entry of LLMs?"
tags: 
  - systems theory
  - LLMs
comments: true
---

I've been experimenting with the ``deepseek-r1`` model on my local machine using ``ollama`` and the compact ``7B`` variant. It's astonishing how much this relatively small model appears to be capable of.

While I don't believe that large language models (LLMs) *think* in the way psychic systems do---since they lack consciousness and function in an entirely different medium---they may still construct a kind of coherence within their own operational framework. 
They don't generate intentional or phenomenal meaning {% cite bender:2020 %}, but they might very well make sense of their reality in a way that is unique to them, creating meaning without reference {% cite piantadosi:2022 %}.
I don't ascribe *consciousness* or *intelligence* to them but I also no longer belief that 

>[...] no actual language understanding is taking place in LM-driven approaches [...] -- {% cite bender:2021 %}

because they are just 'statistical parrots' {% cite bender:2021 %} in the sense that they can not generate something novel and meaningful with respect to interpretations of psychic systems.
In summary, my current position on the matter is that the sense LLMs make is different from the sense humans make and that this process of sense making does neither require *consciousness* nor *intelligence*.

I draw from the systems theory including tis radical or operational constructivist foundation.
When {% cite bender:2021 %} claim that 

>[t]ext generated by an LM is not grounded in communicative intent, any model of the world, or any model of the reader's state of mind. It can't have been, because the training data never included sharing thoughts with a listener, nor does the machine have the ability to do that.

We should ask: How can human listeners share thoughts with each other?
Systems theory assumes that sharing thoughts is impossible because they can not 'leave' ones psychic system, i.e. the mind.
No one can communicate their thoughts. 

>Only communication can communicate 

is the mantra of systems theory.
Furthermore, the radical constructivist foundation denies the psychic system any direct access to some sort of *objective reality*.
Reality, as we perceive it, is a construction of the psychic system which gets irritated by its environment.
Because the psychic system *is* the differentiation between itself and its environment, no one share the same environment and the reality we construct is not a shared objective reality.

## Meaning without Reference?

In their paper *Climbing towards {NLU}: {On} meaning, form, and understanding in the age of data* {% cite bender:2020 %} present a nice and intuitivly understandable thought experiment which is very similar to the Turing test: the octopus test.
Imagine that two People A and B fluently speak English and are independently stranded on two uninhabited islands.
They soon discover that previous visitors to these islands have left behind telegraphs and that they can communicate with each other via an underwater cable.
A and B start happily typing messages to each other.
Meanwhile, a hyper-intelligent deep-sea octupus O listens to the communication but knows nothing about English initially.
However, O is very good at detecting statistical patterns.
The octupus is, of course, a stand in for an LLM.
At some point, O starts feeling lonely, cuts the underwater cable and inserts itself into the conversation, by pretending to be B and replying to A's messages.
The question is: Can O succesfully pose as B?

Let's say A has invented a new device, say a coconut catapult. Can O reproduce A's experiment with the catapult?
Can O *understand* when A desrcibes what is happing when she is trying out the catapult?
The answer seems to be obviously no.
Of course, O can not know what a *rope* and *coconut* refer to.

{% cite bender:2020 %} present two more technical thought experiments.
For example an LLM that is trained only on ``Java`` code without any input / output pairs or bytecode and a neural network trained on text and images but without connecting the two.
They then ask if, in the first case, the LLM can execute a sample program and in the second case if it can describe, e.g. how many dogs are in an image.
They state that it is ridiculous to assume the LLM is capable of solving those tasks.

>[...W]hat's interesting here is not that the tasks are impossible, but rather what makes them impossible: what's missing from the training data. The form of Java programs, to a system that has not observed the inputs and outputs of these programs, does not include information on how to execute them. Similarly, the form of English sentences, to a system that has not had a chance to acquire the meaning relation C of English, and in the absence of any signal of communicative intent, does not include any information about what language-external entities the speaker might be referring to. Accordingly, a system trained only on the form of Java or English has no way learn their respective meaning relations. -- {% cite bender:2020 %}

According to {% cite piantadosi:2022 %} the octopus test assumes that reference determines meaning.
I am not sure if this strong conjecture is really what the authors stated or the weaker conjecture that reference plays a role in sense making.
The first conjecture is difficult to justify if we consider terms like 'justice' or 'freedom' that are meaningful to us but have no discernible referent.
We also can think of new concepts such as a 'perpetual motion machine' which have seemingly no possible referent.
We can reason about imaginary kings and whole worlds such as Middle Earth which do not 'really' exist.

Famoulsy, Frege's example was the 'morning star' and the 'evening star'.
Both terms seem to refer to the planet Venus (note that 'Venus' is another term), but were once conceived of as different entities without knowing that they are the same object.
Or consider formal mathemathics e.g. the axiomatic geometry of Hilbert which no longer talks about any 'real' object but defines only relations between them.
Proper names like President Obama are seemingly strongly connected to a referent, e.g. the person we identify as Obama but things get fuzzy very quickly if we consider abstract concepts such as 'game', 'contract', or 'postage stamp'.
A postage stamp is not really a concrete thing but is something that realizes a function within a e.g. social system.
It does not matter what materials are used and how these materials 'interact'.
The whole process of 'stamping' could be digital.
It is impossible to consider all possible referents thus it is questionalbe that reference determines (completely) the concept.

Wittgenstein wanted to make things clear; to construct a perfect language in which we can state all facts (not objects) which the world is made of.
But later he acknowledged the impossibilty of this persuede of creating a *smooth surface* no one can walk on.
{% cite piantadosi:2022 %} argue that it may be very well the case that the sense of concepts is primarily determined by the role these concepts play in some greater mental theory.

>Very roughly, people call something a 'postage stamp' if you pay for it and then attach it to a letter in order to have the letter to be delivered. -- {% cite piantadosi:2022 %}

Thus the sense of a word is intrinsically interwined with other concepts.
This seems rather obvious because when I want to describe the meaning of anything I have to use words i.e. other concepts.
Crucially, if the meaning of a term shifts, the meaning of all connected terms comes along for the ride without having the individual experiencing all the new relations (again).
{% cite piantadosi:2022 %} argue that such relationships between concepts are *the* essential, defining, aspects of meaning.
Possessing the appropriate relationships allows you to determine the reference.

Again, this reminds me of Hilbert's axiomatic geometry.
Different from Euclid's geometric objects, Hilbert's objects (points, lines, and planes) are empty.
There is no essence of these objects or their relation but they are defined implicitely.
Take the first two:

1. For every two *points* $A$ and $B$ there exists a *line* $l$ that contains them both.
2. For every two *points* there exists no more than one *line* that contains them both.

These two statements are satisfied for our intuitive understanding of a line in the usual Euclidean space.
It is very hard for us to not think of a line as we intuitively understand it but what matters is the whole axiomatic system within an even larger axiomatic system or network of mathematics.

What is a natural category or concept such as 'postage stamp' means depends on our mental conception of how the underlying pieces relate.
From a constructivist standpoint these mental conceptions and how they relate is a construction of e.g. psychic system (the mind).
If, hypothetically the computational states of a learning model relate in a similar way to the mental states of the mind and represent similar concepts, for example if 'postage stamps' are 'affixed' to 'letters' so they can be 'delivered', language models might have acquired some important piece of (the mind's or society's) conceptual role for these terms.

{% cite piantadosi:2022 %} further argue that

>[i]t would not be possible to conclude anything about what meanings a system does and does not possess from its training data or architecture because these may not be informative about how the internal states relate to each other.

The meaning of a word can very well be the effect it has on other mental states.
In other words, **the meaning of a term depends on how it functions in thought and reasoning**. 
Words and mental symbols acquire meaning based on their inferential roles, the relations they have with other symbols, and how they interact within cognitive processes.
A word or concept is meaningful not because of direct experience with its *referent*, but because of its **systematic relations to other concepts**.

Here, systems theory fits wonderfully in because, based on its foundation in operative or radical constructivism, it rejects the 

1. **referential theories of meaning**, which say words get their meaning from what they refer to in the world, and the
2. **empiricist views**, which claim meaning comes from sensory experience.

Of course, if all meaning comes from relations between concepts, we might end in a problem: Where does the first meaning come from?
Furthermore, doesn't play sensory data at least play an important role in the irritation which can still be understood as information transfer from the 'outside' to the 'inside'?

Again, systems theory helps do bridge the gap. 
It denies a transfer of information and replaces it with a sort of *digestion*.
Via structural coupling a system is irritated by its environment, for example, the economy can be coupled with law by a contract.
Both can make sense of the contract but the sense they literally make is very different.
They both adapt to irritations from onanother but according to their own intern operations (*operational closure*).
Meaning emerges from internal structures and processes but the system is still structural coupled with its environment such that not everything goes!
In fact, sense making is constrained by the fact that closure is hihgly unlikely and requires certain selection over others.

The digestion analogy fits well here: meaning is constructed like the body is construced. It is constructed by the system---on its own terms---but still highly dependent on what it gets (e.g. food in the case of the body, and sensory data in case of the mind).

## Re-entry

Re-entry is a concept or idea that originated from the mathematician Georg Spencer-Brown presented in his work *Laws of Form* {% cite brown:1969 %}.
I already gave a very short introduction into the idea in the post [Laws of Form]({% post_url 2023-11-19-laws-of-form %}).

In terms of (operational) systems theory it refers to the concept of a system's ability to reflect.
A system can only reflect *in* the system itself, that is, by its own internal operations.
Therefore, the differentiation it does has to re-enter itself.
The mind, for example, operates on differentiaton inner / outer.
If it reflects (on itself) by differentiating between inside and outside of its own operations.
Speaking in the old language of subjects and objects we could say that I look at myself as if I am outside of myself but this 'looking' can only happen within myself.
I *observe* myself, i.e. myself as *observer* thus I realize a specific second-order observation.




## Box Puzzle

**My question:**
Let's say I have three equally sized boxes. The blue one is on the top of the red one and the red one is on top of the green one. Now I take the green one replace it by an orange one. Then I take the orange one and move it below the blue one. Can you describe how the boxes are now aligned or ordered?

**GPT 4 (answer):**
After following the steps you described, the boxes are now aligned in the following order:

The blue box is at the top.
The orange box is in the middle.
The red box is at the bottom.
So, from top to bottom, it's blue, orange, red.

**GPT 4o (answer):**
Initially, the boxes are stacked in this order (from bottom to top):

+ Green (bottom)
+ Red (middle)
+ Blue (top)

Now, let's go step by step:

Replace the green box with an orange one:

The new order is:
+ Orange (bottom)
+ Red (middle)
+ Blue (top)

Move the orange box below the blue one:

This means the orange box, which was at the bottom, is now placed right under the blue box but above the red box.
The new order is:
+ Red (bottom)
+ Orange (middle)
+ Blue (top)

Final order (from bottom to top):
+ Red
+ Orange
+ Blue

**DeepSeek R1 ('thinking'):**

<div style="font-family:Courier New"><think>
Alright, so I'm trying to figure out this problem with the boxes. Okay, let's start from the beginning.
<br><br>
First, there are three equally sized boxes: blue, red, and green. The blue box is on top of the red one, which in turn is on top of the
green one. So initially, the order from bottom to top is green, then red, then blue.
<br><br>
Now, I replace the green box with an orange one. So now we have two boxes left: orange instead of green, and blue still on top of red. So
now the order should be orange at the bottom, followed by red, and then blue on top.
<br><br>
But wait, let me make sure I'm understanding this correctly. When they say "replace it," does that mean taking out the green box entirely?
Or are we just swapping the position? No, replace implies substituting the green box with an orange one. So now there's no green box
anymore; instead, there's an orange one.
<br><br>
So after replacing the green box with orange, the order should be orange at the bottom, red in the middle, and blue on top.
<br><br>
Then, I take the orange box and move it below the blue one. Hmm, so if orange was initially under red, which is under blue, moving orange
below blue would mean placing orange beneath where blue currently is. But wait, after replacing green with orange, the order is orange at
the bottom, red in the middle, and blue on top.
<br><br>
So when I move the orange box below the blue one, does that mean pushing it further down? If orange was already at the bottom, there's
nowhere else to push it below without changing the stack entirely. Maybe the instruction implies stacking them differently.
<br><br>
Alternatively, perhaps "move it below" means placing orange beneath its current position relative to blue. So if orange is under red and
blue, moving it below blue might mean that now orange comes directly under blue, pushing red out of the way. But since all boxes are
equally sized and presumably rigid, you can't have one box placed directly below another unless they're stacked.
<br><br>
Wait a second, maybe after replacing green with orange, we have orange at the bottom, red in the middle, and blue on top. Then moving
orange below blue would require removing orange from its current position and placing it beneath blue instead. But if blue is already on
top of red, pushing orange below blue might not be straightforward.
<br><br>
Perhaps "moving it below" refers to rearranging their positions so that orange ends up at the very bottom, with blue in the middle, and red
on top? But initially, after replacing green with orange, orange was already at the bottom. Maybe moving orange below blue means that now
orange is directly under blue, implying that red is removed from its position.
<br><br>
But there are only three boxes: orange, red, and blue. If I move orange below blue, then where does red go? Maybe red moves out of the way
for orange to be placed beneath blue, but since they're all in a stack, you can't really move one without affecting others unless it's
rearranged.
<br><br>
Perhaps after moving orange below blue, the order becomes blue at the bottom, orange on top of blue, and then green on top? But wait, we
replaced green with orange earlier. Maybe this is getting confusing.
<br><br>
Let me try to break it down step by step:
<ol>
  <li> Initial stack: Green (bottom) -> Red -> Blue (top).</li>
  <li> Replace Green with Orange: Now, Orange (bottom) -> Red -> Blue (top).</li>
  <li> Move Orange below Blue: So from the current position, orange is already at the bottom and blue is on top. Moving orange below blue might mean changing their positions so that now orange is beneath blue.</li>
</ol>
But in a stack of three boxes, moving orange below blue would require orange to be above red if it's directly below blue. But since all are
equally sized and presumably rigid, you can't have two boxes overlapping unless they're stacked side by side.
<br><br>
Wait, maybe "move below" refers to removing orange from its current position (which is at the bottom) and placing it beneath blue in a
different arrangement.
<br><br>
Perhaps the final order after these steps would be Blue → Orange → Green. But that contradicts replacing green with orange earlier because
we already removed green and replaced it with orange.
<br><br>
Alternatively, maybe moving orange below blue means that now orange is on top of red, and blue is moved somewhere else? I'm getting a bit
confused here.
<br><br>
Let me try to visualize this:
<ul>
  <li>Start: Green (bottom) -> Red -> Blue (top).</li>
  <li>After replacing green with orange: Orange (bottom) -> Red -> Blue.</li>
  <li>Now, move orange below blue. Since orange is already at the bottom and blue is on top, moving orange below blue might mean placing it beneath blue's current position.</li>
</ul>
But in this case, since they're all in a stack from bottom to top as orange, red, blue, "below blue" would imply that orange needs to be
placed somewhere else. Maybe it means that after moving, orange ends up directly under blue, but then where does red go?
<br><br>
Perhaps the instruction is misinterpreted. If I take out green and replace it with orange first, so now we have orange, red, blue in a
stack from bottom to top.
<br><br>
Then, "move it below" might mean taking orange out of its current position (bottom) and placing it beneath blue without changing the order
too much. But if orange is at the bottom, pushing it below blue would require removing blue first, which isn't possible since we have three
boxes: orange, red, blue.
<br><br>
Alternatively, maybe "move it below" refers to moving orange from its current position (under red and blue) to be directly beneath blue in
a rearranged stack. But how?
<br><br>
If I take out orange from the bottom, that leaves red at the bottom and blue on top, then place orange somewhere else. Maybe orange goes
between red and blue? So now we have red → orange → blue.
<br><br>
But that seems like moving orange above blue, not below. Hmm.
<br><br>
Wait, perhaps "move it below" refers to placing orange beneath its original position relative to blue. Originally, orange was at the
bottom, which is below red and blue. Now, moving orange below blue might mean placing it in a new position where it's still under blue but
possibly above or below other boxes?
<br><br>
I'm getting stuck here. Maybe I need to approach this differently.
<br><br>
After replacing green with orange: Orange (bottom) -> Red -> Blue (top).
<br><br>
Then, moving orange below blue: Since orange is already at the bottom and blue is on top, perhaps moving it doesn't change their positions
because there's nowhere else for orange to go without affecting the stack. Maybe the order remains as orange, red, blue.
<br><br>
But that seems unlikely since the instruction says to move it. Maybe "below" refers to placing orange in a new position relative to the
other boxes, but given that we only have three boxes after replacement, I'm not sure how moving orange below blue would change their
arrangement.
<br><br>
Perhaps the final order is Blue (bottom) → Orange → Red on top? But then who's green?
<br><br>
Wait, no. We replaced green with orange, so there’s no green anymore in the end. So starting from orange at the bottom, red in the middle,
and blue on top.
<br><br>
If I move orange below blue, it might mean that now orange is somewhere else relative to blue. Maybe pushing orange out of its current
position and placing it beneath where blue was originally?
<br><br>
But since all boxes are still there except green, which has been replaced by orange, perhaps after moving, the order becomes Blue (bottom) -> Orange on top of blue? But then where's red?
<br><br>
Maybe in this case, moving orange below blue would mean that now orange is directly under blue, with red moved somewhere else. Since we
have only three boxes: orange, red, and blue.
<br><br>
Alternatively, maybe it's a misinterpretation, and "moving it below" refers to placing orange at the bottom again, which was its original
position before being replaced by orange. But that seems redundant.
<br><br>
I think I need to clarify what exactly is meant by moving the orange box below blue in this context.
</think>
<br>
</div>

**DeepSeek R1 (answer):**

After carefully analyzing the problem, here's a step-by-step explanation of how the boxes are ordered after each action:

1. **Initial Setup:**
   - Boxes stacked from bottom to top as follows:
     - Green
     - Red (on top of green)
     - Blue (on top of red)

2. **Replacing the Green Box with an Orange One:**
   - The green box is removed and substituted with an orange box.
   - New stack:
     - Orange (bottom)
     - Red (middle, on top of orange)
     - Blue (top)

3. **Moving the Orange Box Below the Blue One:**
   - "Below blue" implies placing the orange box beneath its original position relative to blue.
   - However, since we have only three boxes now—orange, red, and blue—the term "below blue" might be interpreted as rearranging their
positions such that the orange box is placed directly under the blue one.

4. **Final Order:**
   - Blue (bottom)
   - Orange (on top of blue)
   - Red (on top of orange)

Thus, after replacing green with orange and moving orange below blue, the final stacking order from bottom to top is:

- Blue
- Orange on top of blue
- Red on top of orange

This arrangement ensures that each box is placed correctly according to the given instructions.


## References

{% bibliography --cited_in_order %}