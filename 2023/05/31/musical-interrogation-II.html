<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Musical Interrogation II - MC and FFN | Bene’s Blog</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Musical Interrogation II - MC and FFN" />
<meta name="author" content="Benedikt Zönnchen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The code shown here can be found in the following GitHub repository. In the second installment of this series, I introduce an initial and arguably the most basic method to generate monophonic melodies. This consists of two approaches: (1) A first-order Markov chain (MC) and (2) a feedforward neural network (FNN)" />
<meta property="og:description" content="The code shown here can be found in the following GitHub repository. In the second installment of this series, I introduce an initial and arguably the most basic method to generate monophonic melodies. This consists of two approaches: (1) A first-order Markov chain (MC) and (2) a feedforward neural network (FNN)" />
<link rel="canonical" href="https://bzoennchen.github.io/Pages/2023/05/31/musical-interrogation-II.html" />
<meta property="og:url" content="https://bzoennchen.github.io/Pages/2023/05/31/musical-interrogation-II.html" />
<meta property="og:site_name" content="Bene’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-05-31T00:00:00+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Musical Interrogation II - MC and FFN" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Benedikt Zönnchen"},"dateModified":"2023-05-31T00:00:00+02:00","datePublished":"2023-05-31T00:00:00+02:00","description":"The code shown here can be found in the following GitHub repository. In the second installment of this series, I introduce an initial and arguably the most basic method to generate monophonic melodies. This consists of two approaches: (1) A first-order Markov chain (MC) and (2) a feedforward neural network (FNN)","headline":"Musical Interrogation II - MC and FFN","mainEntityOfPage":{"@type":"WebPage","@id":"https://bzoennchen.github.io/Pages/2023/05/31/musical-interrogation-II.html"},"url":"https://bzoennchen.github.io/Pages/2023/05/31/musical-interrogation-II.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="shortcut icon" href="https://bzoennchen.github.io/Pages/favicon.ico" />
  <link rel="stylesheet" href="/Pages/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://bzoennchen.github.io/Pages/feed.xml" title="Bene&apos;s Blog" /><link type="application/atom+xml" rel="alternate" href="https://bzoennchen.github.io/Pages/feed.xml" title="Bene&apos;s Blog" />
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond&family=Noto+Serif:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
<!--<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>-->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
  type="text/javascript">
</script>--></head>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <body><header class="site-header">
    <div class="wrapper"><a class="site-title" rel="author" href="/Pages/">Bene&#39;s Blog</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
  
          <div class="trigger">
              
              <a class="page-link" href="/Pages/">About</a>
              
              <a class="page-link" href="/Pages/posts/">Posts</a>
              
              <a class="page-link" href="/Pages/works/">Selected Works</a>
              
              <a class="page-link" href="/Pages/cv/">CV</a>
              
              <a class="page-link" href="/Pages/contact/">Contact</a>
              

            <!--<a class="page-link" href="/Pages/about/">About</a><a class="page-link" href="/Pages/contact/">Contact</a><a class="page-link" href="/Pages/cv/">CV</a><a class="page-link" href="/Pages/">About</a><a class="page-link" href="/Pages/posts/">Posts</a><a class="page-link" href="/Pages/animations/">Projects</a><a class="page-link" href="/Pages/works/">Selected Works</a>-->
          </div>
        </nav></div>
  </header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Musical Interrogation II - MC and FFN</h1>
    <p class="post-meta"><time class="dt-published" datetime="2023-05-31T00:00:00+02:00" itemprop="datePublished">
        May 31, 2023
      </time><span class="tag-list">|
      
        Music, 
        ML, 
        FFN, 
        MC
      </span></p>
  </header>

 

  
  
  
  
<div class="panel seriesNote">
    <p>
        This article is <strong>Part 2</strong> in a <strong>3-Part</strong> Series.
    </p>
    <ul>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li>Part 1 -
            
            <a href="/Pages/2023/04/02/musical-interrogation-I.html">Musical Interrogation I - Intro</a>
            
        </li>
        
        
        
        
        
        
        <li>Part 2 -
            
            Musical Interrogation II - MC and FFN
            
        </li>
        
        
        
        
        
        
        
        
        <li>Part 3 -
            
            <a href="/Pages/2023/11/19/musical-interrogation-III.html">Musical Interrogation III - LSTM</a>
            
        </li>
        
        
        
        
    </ul>
</div>



  <div class="post-content e-content" itemprop="articleBody">
    <p>The code shown here can be found in the following GitHub <a href="https://github.com/BZoennchen/musical-interrogation">repository</a>.
In the second installment of this series, I introduce an initial and arguably the most basic method to generate monophonic melodies. 
This consists of two approaches: (1) A <strong>first-order Markov chain (MC)</strong> and (2) a <strong>feedforward neural network (FNN)</strong></p>

<p>It’s important to note that I will disregard all forms of dynamics within a notated score (or performace), such as loudness, softness, etc.</p>

<p>Despite these two approaches being significantly outdated, I believe their demonstration serves as a valuable exercise for familiarizing oneself with the inherent challenges of the subject matter. 
The inspiration for this work comes from the tutorial series made by <a href="https://www.youtube.com/watch?v=FLr0r-QhqH0&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz">Valerio Velardo’s</a> and another series made by <a href="https://www.youtube.com/@AndrejKarpathy">Andrej Karpathy’s</a>.</p>

<p>Although I utilize high-level libraries such as <a href="https://pytorch.org/">PyTorch</a> and take advantage of its <em>computational graph</em> and <em>autograd</em> features, I intend to maintain the model code and training process at a relatively low level.</p>

<h2 id="requirements">Requirements</h2>

<p>The necessary software requirements for this project include:</p>

<ul>
  <li><a href="https://www.python.org/">Python</a></li>
  <li><a href="https://pytorch.org/">PyTorch</a></li>
  <li><a href="http://web.mit.edu/music21/">Music21</a></li>
  <li><a href="https://github.com/BZoennchen/musical-interrogation/blob/main/preprocess.py">preprocessor.py</a> helpler class to deal with <code class="language-plaintext highlighter-rouge">krn</code>-files.</li>
  <li><a href="https://musescore.org/de">MuseScore</a> (optional)</li>
  <li><a href="https://jupyter.org/">Jupyter Notebook Environment</a> (optional)</li>
</ul>

<p>The database required can be found at <a href="http://kern.ccarh.org">EsAC</a>. The specific dataset I utilized is <a href="https://kern.humdrum.org/cgi-bin/ksdata?l=/essen/europa&amp;format=recursive">Folksongs from the continent of Europe</a> and for the purpose of this work, I will exclusively use the 1700 pieces found in the <code class="language-plaintext highlighter-rouge">./deutschl/erk</code> directory.</p>

<p>Let’s listen to one of these pieces:</p>

<audio controls="">
  <source src="/Pages/assets/audio/mel0567.mp3" type="audio/mp3" />
  Your browser does not support the audio element.
</audio>

<h2 id="data-representation">Data Representation</h2>

<p>In <a href="/Pages/2023/04/02/musical-interrogation-I.html">Part I</a> of this series, I alluded to various implementations that utilize different input encodings. Naturally, the information we can leverage depends on the format of our training data. 
For instance, MIDI provides us with attributes such as pitch, duration, and velocity.
In my <a href="">implementation</a>, you will notice two distinct, yet straightforward encoding options available.</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">GridEncoder</code> used by Valerio Velardo</li>
  <li><code class="language-plaintext highlighter-rouge">NoteEncoder</code></li>
</ol>

<p>The <code class="language-plaintext highlighter-rouge">GridEncoder</code> utilizes a fixed metrical grid where (a) output is generated for every timestep, and (b) the step size corresponds to a fixed meter (the shortest duration of any note in any score). For instance, if the shortest duration is a quarter note, a whole note of pitch 65 (in MIDI format) would result in the event series:</p>

<p><strong>65-note-on hold hold hold</strong></p>

<p>On the other hand, the <code class="language-plaintext highlighter-rouge">NoteEncoder</code> employs a larger alphabet and encodes one note directly, i.e.,</p>

<p><strong>65-whole</strong></p>

<p>In comparison to <em>note encoding</em>, <em>equitemporal grid encoding</em> relies on a smaller alphabet but needs more tokens for the same score.
This disadvantage is magnified if the score contains notes of vastly differing durations or if we wish to introduce micro-dynamics through an increase in resolution, as done by <a class="citation" href="#oore:2018">(Oore et al., 2018)</a>.</p>

<p>Interestingly, Google’s <a href="https://magenta.tensorflow.org/">Magenta project</a> employs <em>equitemporal grid encoding</em>, specifically the <a href="https://github.com/magenta/note-seq/blob/main/note_seq/melody_encoder_decoder.py">MelodyOneHotEncoding</a> class for their <em>BasicRNN</em>, <em>MonoRNN</em>, and <em>LookbackRNN</em>. 
Since they capture polyphonic scores, they utilize <strong>note-on</strong> and <strong>note-off</strong> events for each MIDI key.</p>

<p>Of course, the chosen representation also depends on the application and the capabilities of the model we use. 
For instance, one might only want to generate the pitches of the melody and manually adjust the duration of each note in post-processing. 
Furthermore, a <em>first-order Markov chain</em> only <em>memorizes</em> the most recent event. 
Therefore, an <em>equitemporal grid encoding</em> would yield unsatisfactory results because the context is lost after a <strong>hold</strong> event occurs.</p>

<p>As a result, in this post, I will focus on the <em>note encoding</em> approach, i.e. <code class="language-plaintext highlighter-rouge">NoteEncoder</code>.</p>

<h2 id="preprocessing">Preprocessing</h2>

<p>The procedure I’m about to present parallels the one detailed in <a href="/Pages/2022/07/09/markov-chains-for-music-generation.html">Probabilistic Melody Modeling</a>. 
The primary differences are that we’ll now be considering 1700 pieces instead of a single one, and we utilize more sophisticated libraries instead of relying solely on <a href="https://sonic-pi.net/">Sonic Pi</a>.</p>

<p>The <a href="http://web.mit.edu/music21/">Music21</a> library significantly simplifies the handling of symbolically notated music in <code class="language-plaintext highlighter-rouge">Python</code>.
I am not so familiar with it but it comes in handy when reading and writing symoblic scores.
It enables us to construct pieces programmatically and to read from or write to various musical score formats.</p>

<p>As an initial step, we need to import all the necessary libraries and functions.
Here I fix the global seed such that you can reproduce the exact same results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">music21</span> <span class="k">as</span> <span class="n">m21</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">preprocess</span> <span class="kn">import</span> <span class="n">load_songs_in_kern</span><span class="p">,</span> <span class="n">NoteEncoder</span><span class="p">,</span> <span class="n">KERN_DATASET_PATH</span>

<span class="c1"># seed such that we can compare results
</span><span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</code></pre></div></div>

<p>Then I read all the pieces inside the <code class="language-plaintext highlighter-rouge">./../deutschl/erk</code> directory.
Furthermore, I introduce a special character <code class="language-plaintext highlighter-rouge">TERM_SYMBOL</code> that I use to indicate the beginning and end of a score.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TERM_SYMBOL</span> <span class="o">=</span> <span class="s">'.'</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">load_songs_in_kern</span><span class="p">(</span><span class="s">'./../deutschl/erk'</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we have to think about our encoding.
As discussed above, I use the <code class="language-plaintext highlighter-rouge">NoteEncoder</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">encoder</span> <span class="o">=</span> <span class="n">NoteEncoder</span><span class="p">()</span>
<span class="n">enc_songs</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">.</span><span class="n">encode_songs</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">enc_songs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p>The code above prints out:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'55/4 60/4 60/4 60/4 60/4 64/4 64/4 r/4 ... 64/4 60/4 62/4 60/8 r/4'
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">'55/4</code> means MIDI note 55 four timesteps long where the timestep is determined by the shortest note within all scores. 
In our case this means four times 1/4 beat which is one whole beat.</p>

<p>Given that computers cannot process strings directly, I convert these strings into numerical values. 
The first step is to create a set that includes all possible strings. Subsequently, I assign each string a corresponding natural number in sequential order.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">symbols</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">([</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> 
                           <span class="n">enc_songs</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">])))</span>
<span class="n">stoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">symbols</span><span class="p">)}</span>
<span class="n">stoi</span><span class="p">[</span><span class="n">TERM_SYMBOL</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">itos</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">stoi</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'n_symbols: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">itos</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">stoi</code> maps <strong>s</strong>trings <strong>to</strong> <strong>i</strong>ntegers and <code class="language-plaintext highlighter-rouge">itos</code> is its inverse mapping.</p>

<h2 id="discrete-first-order-markov-chain">Discrete First-Order Markov Chain</h2>

<p>To implement a <em>first-order Markov chain</em>, we aim to construct a Markov matrix</p>

\[\mathbf{P} \in [0;1]^{m \times m}\]

<p>where the element at the \(i^{\text{th}}\) row and \(j^{\text{th}}\) column represents the conditional probability</p>

\[P(e_i\ | \ e_j) = p_{ij}.\]

<p>It describes the (conditional) probability of event \(e_i\) (a note or rest of specific length) immediately following event \(e_j\).
For this purpose, I construct a matrix \(\mathbf{N}\) that counts these transitions.</p>

<h3 id="markov-matrix-computation">Markov Matrix Computation</h3>

<p>To accomplish this, I iterate over each score, considering every pair of consecutive events. 
As the first event lacks a predecessor and the last lacks a successor, I append the unique terminal character <code class="language-plaintext highlighter-rouge">TERM_SYMBOL</code> to each score for padding purposes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">)))</span>
<span class="k">for</span> <span class="n">enc_song</span> <span class="ow">in</span> <span class="n">enc_songs</span><span class="p">:</span>
    <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="n">TERM_SYMBOL</span><span class="p">]</span> <span class="o">+</span> <span class="n">enc_song</span> <span class="o">+</span> <span class="p">[</span><span class="n">TERM_SYMBOL</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
        <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
        <span class="n">N</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<p>To construct \(\mathbf{P}\) we have to divide each entry \(n_{ij}\) in \(\mathbf{N}\) by the sum over the row \(i\).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">P</span> <span class="o">=</span> <span class="n">N</span><span class="p">.</span><span class="nb">float</span><span class="p">()</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">P</span> <span class="o">/</span> <span class="n">P</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>In order to compute the sum over a row (instead of a column), i.e., “summing all columns”, we need to specify <code class="language-plaintext highlighter-rouge">dim=1</code> (the default is <code class="language-plaintext highlighter-rouge">dim=0</code>). 
Additionally, to properly exploit broadcasting, it’s necessary to set <code class="language-plaintext highlighter-rouge">keepdim=True</code>. 
This ensures that the sum results in a <code class="language-plaintext highlighter-rouge">(1,m)</code> tensor, as opposed to a <code class="language-plaintext highlighter-rouge">(m,)</code> tensor.</p>

<p>Plotting the probabilities reviels that \(\mathbf{P}\) is a rather sparse matrix containing many zeros.
In fact, only approximately 7.86 percent of the entries are non-zero.</p>

<div><img style="display:block; margin-left:auto; margin-right:auto; width:70%;" src="/Pages/assets/images/mc-probs.png" alt="Probabilities" />
<div style="display: table;margin: 0 auto;">Figure 1: Matrix plot of our Markov matrix.</div>
</div>
<p><br /></p>

<h3 id="sampling-of-new-melodies">Sampling of New Melodies</h3>

<p>Given the tensor <code class="language-plaintext highlighter-rouge">P</code>, we can generate new melodies using the function <code class="language-plaintext highlighter-rouge">torch.multinomial</code> which expects a probability (discrete) distribution.
I start with the terminal <code class="language-plaintext highlighter-rouge">TERM_SYMBOL</code> indicating the beginning and, when the second terminal is generated (which indicates the end), I terminate the generation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">generated_encoded_song</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">char</span> <span class="o">=</span> <span class="n">TERM_SYMBOL</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">P</span><span class="p">[</span><span class="n">stoi</span><span class="p">[</span><span class="n">char</span><span class="p">]],</span> 
        <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
        <span class="n">replacement</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
    <span class="n">char</span> <span class="o">=</span> <span class="n">itos</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">char</span> <span class="o">==</span> <span class="n">TERM_SYMBOL</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">generated_encoded_song</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">generated_encoded_song</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s listen to some of the generated scores:</p>

<audio controls="">
  <source src="/Pages/assets/audio/gen0.mp3" type="audio/mp3" />
  Your browser does not support the audio element.
</audio>

<audio controls="">
  <source src="/Pages/assets/audio/gen1.mp3" type="audio/mp3" />
  Your browser does not support the audio element.
</audio>

<audio controls="">
  <source src="/Pages/assets/audio/gen2.mp3" type="audio/mp3" />
  Your browser does not support the audio element.
</audio>

<audio controls="">
  <source src="/Pages/assets/audio/gen3.mp3" type="audio/mp3" />
  Your browser does not support the audio element.
</audio>

<audio controls="">
  <source src="/Pages/assets/audio/gen4.mp3" type="audio/mp3" />
  Your browser does not support the audio element.
</audio>

<h3 id="negative-log-likelihood-loss">Negative Log Likelihood Loss.</h3>

<p>The outcome is not particularly outstanding, but this is unsurprising given our very simple model. 
To evaluate the quality of our model, we can calculate the likelihood that our generative process produces for a specific training data point \(e_1, e_2, \ldots, e_k\), i.e.,</p>

\[P(e_1) \cdot P(e_2 \ | \ e_1) \cdot \ldots \cdot P(e_{k-1} \ | \ e_k).\]

<p>We can add all the likelihoods (one for each data point) together and divide the sum by the number of data points.
However, it is more convinient to use the <em>negative log-likelihood</em> since one can use addition.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">log_likelyhood</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">enc_songs</span><span class="p">:</span>
    <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="n">TERM_SYMBOL</span><span class="p">]</span> <span class="o">+</span> <span class="n">m</span> <span class="o">+</span> <span class="p">[</span><span class="n">TERM_SYMBOL</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
        <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">P</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span> <span class="n">ix2</span><span class="p">]</span>
        <span class="n">logprob</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
        <span class="n">log_likelyhood</span> <span class="o">+=</span> <span class="n">logprob</span>
        <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">log_likelyhood</span><span class="o">=</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="n">nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_likelyhood</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'avg negative log likelyhood: </span><span class="si">{</span><span class="p">(</span><span class="n">nll</span><span class="o">/</span><span class="n">n</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p>This gives us a <em>negative log-likelihood</em> of approximately <code class="language-plaintext highlighter-rouge">2.6756</code>.
The lower this value gets the better it is.
It can be no smaller than 0.</p>

<h2 id="feedforward-neural-network">Feedforward Neural Network</h2>

<p>One method of generating a melody using a <em>feedforward network (FFN)</em> is by addressing a classification task.
Well, strictly speaking we do not even build a FFN, since there will be no activation function involved thus it is a <strong>linear model</strong>.
However, from this starting point we could expand our model into a <em>multi-layer perceptron (MLP)</em>.
By avoiding the activation function, it is easier for me to explain exactly what is going on.</p>

<p>If we think in terms of classification a sequence of note should be classified as some successor note.
So let us assume \(t\) consecutive notes are given then our aim is to identify the note that this sequence “represents”.
For simplicity, I assume we only want to predict the next note given the previous one, that is, \(t=1\) holds.
This stipulation means we won’t require substantial modifications compared to our previous approach.</p>

<p>Since our training process will be more computationally intensive than merely computing frequencies, it’s advisable to use hardware accelerators, if available. 
This can result in faster training and inference times and lower energy costs.
To check if hardware acceleration is available, I employ the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'mps'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>
    
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">device</span><span class="o">=</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p>What we are going to implement is a <em>fully-visible softmax belief network</em> which only predicts the very next note given the previous note.
However, instead of multiplying \(x_1\) to predict \(x_2\) via</p>

\[h_{\theta}(x)_{j=1,\ldots,m} = \left(\alpha_0^{(j)} + \alpha_1^{(j)} \cdot x \right)_{j=1,\ldots,m},\]

<p>we do something similar but not quite the same.
We <em>hot-encode</em> our alphabet \(\mathcal{X}\) of notes into</p>

\[|\mathcal{X}| = m\]

<p>vectors \(\mathbf{x}_j = (x_1, \ldots, x_m)\), \(j=1, \ldots, m\) of length \(m\), such that,</p>

\[h_{\theta}(\mathbf{x}_i)_{j=1,\ldots,m} = \left( w_{1}^{(j)} \cdot x_1 + w_{2}^{(j)} \cdot x_2 + \ldots + w_{m}^{(j)} \cdot x_m \right)_{j=1,\ldots,m} = \left( w_{i}^{(j)} x_i \right)_{j=1,\ldots,m}.\]

<p>In other words, we compute an embedding of our alphabet/domain of notes such that each note is encoded by an \(m\)-dimensional vector.
This vector represents the probabilities of the next note.
To transform these embeddings into probabilities we apply the softmax function.
Our network is depicted in Fig. 2.
Note that we do not use a bias term, i.e., there is no replacement for \(\alpha_0^{(j)}\).</p>

<p><br /></p>
<div style="display:block; margin-left:auto; margin-right:auto; width:75%;">
<img style="display:block; margin-left:auto; margin-right:auto; width:75%;" src="/Pages/assets/images/fvsbn-hot.png" alt="Sketch of an HMM." />
<div style="display: table;margin: 0 auto;">Figure 2: Fully-visible softmax belief network (multi-class classification with 4 classes) predicting only the next note utilizing hot-encoding. Only one input node fires 1 all the others fire 0. The illustration indicates that the second input is active while all other inptus are inactive.</div>
</div>
<p><br /></p>

<p>Similar as before, our loss is the empirical mean of the negative log likelyhood</p>

\[-\frac{1}{M} \sum_{i=1,j=y_i}^M \log(\sigma(\mathbf{o})_j),\]

<p>where \(j\) is the \(j^\text{th}\) note in our alphabet and \(y_i\), that is \((\mathbf{x}, y_j)\) is in our training data set.</p>

<h3 id="training-data-construction">Training Data Construction</h3>

<p>Instead of calculating our probability matrix, I am going to generate labeled training data using the variables <code class="language-plaintext highlighter-rouge">xs</code> and <code class="language-plaintext highlighter-rouge">ys</code> (labels).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">enc_songs</span><span class="p">:</span>
    <span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="n">TERM_SYMBOL</span><span class="p">]</span> <span class="o">+</span> <span class="n">m</span> <span class="o">+</span> <span class="p">[</span><span class="n">TERM_SYMBOL</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ch1</span><span class="p">,</span> <span class="n">ch2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">chs</span><span class="p">,</span> <span class="n">chs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="n">ix1</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch1</span><span class="p">]</span>
        <span class="n">ix2</span> <span class="o">=</span> <span class="n">stoi</span><span class="p">[</span><span class="n">ch2</span><span class="p">]</span>
        <span class="n">xs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix1</span><span class="p">)</span>
        <span class="n">ys</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix2</span><span class="p">)</span>

<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># one-hot-encoding
</span><span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">)).</span><span class="nb">float</span><span class="p">()</span>
</code></pre></div></div>

<p>As mentioned, I employ a <em><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">one-hot encoding</a></em> for the input data.
That is, for encoding \(m\) unique elements one uses \(m\) unique \(m\)-dimensional vectors.
One component of these vectors is set to 1.0 and all others are 0.0.
<code class="language-plaintext highlighter-rouge">F.one_hot</code> assumes that our alphabet consists of whole numbers between 0 and \(m-1\), compare the <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html">documentation</a>.</p>

\[(0, \ldots, 0, 1, 0, \ldots, 0)\]

<p>The \(i^{\text{th}}\) element is represented by a vetor where the \(i^{\text{th}}\) component is 1.0.
Note that our labels <code class="language-plaintext highlighter-rouge">ys</code> are not one-hot encoded.</p>

<h3 id="training">Training</h3>

<p>Next, I initialize a random matrix \(\mathbf{W} \in [-1;1]^{m \times m}\), or tensor, <code class="language-plaintext highlighter-rouge">W</code> with values ranging from -1.0 to 1.0.
This tensor includes our trainable parameters, which represent the single layer of our neural network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">)),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<p>Our network includes \(m\) inputs and outputs, with the <em><a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a></em> values of the outputs being interpreted as probabilities.
Essentially, our “network” is just one large matrix!</p>

<p>The operation <code class="language-plaintext highlighter-rouge">xenc @ W</code> represents a matrix multiplication where <code class="language-plaintext highlighter-rouge">xenc</code> is an \(1700 \times m\) matrix and <code class="language-plaintext highlighter-rouge">W</code> is our \(m \times m\) matrix.
Here I use the power of parallel computation.
By employing <code class="language-plaintext highlighter-rouge">probs[torch.arange(len(ys), device=device), ys]</code>, I address a single entry for each row.</p>

<p>Please note that <code class="language-plaintext highlighter-rouge">probs[:, ys]</code> does not work; instead of addressing a single entry, it addresses whole columns indexed by <code class="language-plaintext highlighter-rouge">ys</code>!
Also, be aware that I apply an unusually large learning rate.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training aka gradient decent
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">2_000</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># forward pass
</span>    <span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>
    <span class="n">odds</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">exp</span><span class="p">()</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">odds</span> <span class="o">/</span> <span class="n">odds</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">ys</span><span class="p">].</span><span class="n">log</span><span class="p">().</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'epoch </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    
    <span class="c1"># backward pass
</span>    <span class="n">W</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># set gradients to zero
</span>    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    
    <span class="c1"># update
</span>    <span class="n">W</span><span class="p">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">10.0</span> <span class="o">*</span> <span class="n">W</span><span class="p">.</span><span class="n">grad</span>
</code></pre></div></div>

<p>One iteration of the loop consist of the</p>

<ol>
  <li><em>forward pass</em></li>
  <li><em>backward pass</em> (backwardpropagation) done via <code class="language-plaintext highlighter-rouge">loss.backward()</code> and</li>
  <li>an update of our parameters done via <code class="language-plaintext highlighter-rouge">W.data += -10.0 * W.grad</code>.</li>
</ol>

<p><code class="language-plaintext highlighter-rouge">loss.backward()</code> applies <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> thus computes the gradients and we can update <code class="language-plaintext highlighter-rouge">W</code> by</p>

\[\mathbf{W} \leftarrow \mathbf{W} - \eta \cdot \nabla_\mathbf{W} L\]

<p>where \(\eta = 10\) is the <em>learning rate</em>.</p>

<p>After the initial 2000 epochs the loss is approximately <code class="language-plaintext highlighter-rouge">2.865</code>. 
This performance is somewhat inferior compared to the results achieved by our <em>Markov chain</em>. 
However, by prolonging the training period, I managed to reduce the loss to around <code class="language-plaintext highlighter-rouge">2.707</code>.</p>

<h3 id="what-is-going-on">What is going on?</h3>

<p>Let us assume we have only one sample \(\mathbf{x}\).</p>

<h4 id="forward-pass">Forward Pass:</h4>

<p>The <em>forward pass</em> starts with</p>

\[\mathbf{o} = \mathbf{x} \cdot \mathbf{W}\]

<p>where \(\mathbf{x}\) is a <em>one-hot encoded</em> training data point.
\(\mathbf{o}\) gets interpreted as (component-wise) logarithm of the odds</p>

\[\mathbf{o} = \ln\left(\frac{\mathbf{p}}{\mathbf{1}-\mathbf{p}}\right)\]

<p>which is the <a href="https://en.wikipedia.org/wiki/Logit">logit</a>, i.e., the inverse of the <em>standard logistic function</em> also called <em>sigmoid</em>.
In fact, each data point in \(\mathbf{x}\) selects one row of \(\mathbf{W}\)</p>

\[\mathbf{o} = \mathbf{x} \cdot \mathbf{W} = \begin{bmatrix} o_1 &amp; o_2 &amp; \ldots &amp; o_m \end{bmatrix}.\]

<p>To compute “probabilities” we compute the <em><a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a></em> (<code class="language-plaintext highlighter-rouge">probs</code>) of \(\mathbf{o}\), i.e.,</p>

\[\sigma(\mathbf{o}) = \begin{bmatrix} \sigma(\mathbf{o})_1 &amp; \sigma(\mathbf{o})_2 &amp; \ldots &amp; \sigma(\mathbf{o})_m \end{bmatrix}\]

<p>with</p>

\[\sigma(\mathbf{o})_i = \frac{e^{o_i}}{\sum e^{o_j}}.\]

<p>Luckly the <em>softmax</em> has a simple derivative:</p>

\[\frac{\partial \sigma(\mathbf{o})_i}{\partial o_j} =
\begin{cases}
    \sigma(\mathbf{o}_k)_i - \sigma(\mathbf{o})_i^2 &amp; \text{ if } i = j \\
    -\sigma(\mathbf{o}_k)_i \sigma(\mathbf{o})_j &amp; \text{ otherwise.}
\end{cases}\]

<p>We can also compute the full Jacobian of the <em>softmax</em> vector-to-vector operation:</p>

\[\nabla_{\mathbf{o}} \sigma = \mathbf{J}_{\mathbf{o}}(\sigma) =
\begin{bmatrix} 
    \sigma_1 - \sigma_1^2 &amp; -\sigma_1 \sigma_2 &amp; \ldots &amp; - \sigma_1 \sigma_m  \\
    -\sigma_2 \sigma_1 &amp; s_2 - \sigma_2^2 &amp; \ldots &amp; -\sigma_2 \sigma_m \\
    \ldots &amp; \ldots &amp; \ldots &amp; \ldots \\
    -\sigma_m \sigma_1 &amp; -\sigma_m \sigma_2 &amp; \ldots &amp; \sigma_m - \sigma_m^2
\end{bmatrix} = \text{diag}\left(\sigma\right) - \sigma^{\top} \sigma\]

<p>Similar then before, our loss $L$ is the mean <em>negative log likelihood</em>.</p>

\[L(\mathbf{y}, \sigma) = -\sum\limits_{i=1}^{m} \log(s_i) = -\mathbf{y} \log(\sigma)^\top\]

<p>where \(\mathbf{y}\) is the one-hot encoded label vector, i.e.,</p>

<p><code class="language-plaintext highlighter-rouge">loss = -probs[torch.arange(len(ys), device=device), ys].log().mean()</code>.</p>

<p>Note that \(\mathbf{y}\) is a one-hot encoded vector, <code class="language-plaintext highlighter-rouge">ys</code> is not.</p>

<h4 id="backword-pass">Backword Pass:</h4>

<p>For the <em><a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a></em> we need</p>

\[\nabla_{\mathbf{W}} L(\mathbf{y}, \sigma) = \nabla_{\mathbf{o}} L(\mathbf{y}, \sigma) \cdot \nabla_{\mathbf{W}} \mathbf{o}.\]

<p>Here we employ the <em>chain rule</em>.
The sensitivity of cost \(L\) to the input to the softmax layer, \(\mathbf{o}\) is given by a gradient-Jacobian product, each of which we’ve already computed:</p>

\[\begin{align}
    \nabla_{\mathbf{o}} L(\mathbf{y}, \sigma) &amp;= -\nabla_{\mathbf{o}} \mathbf{y} \log(\sigma)^\top \\
    &amp;= -\mathbf{y} \nabla_{\mathbf{o}}\log(\sigma) \\
    &amp;= -\frac{\mathbf{y}}{\sigma} \nabla_{\mathbf{o}} \sigma \\
    &amp;= -\frac{\mathbf{y}}{\sigma} \cdot \mathbf{J}_{\mathbf{o}}(\sigma) \\
    &amp;= -\frac{\mathbf{y}}{\sigma} \cdot \left[ \text{diag}(\sigma) - \sigma^\top \sigma \right] \\
    &amp;= \frac{\mathbf{y}}{\sigma} \sigma^\top \sigma - \frac{\mathbf{y}}{\sigma} \text{diag}(\sigma) \\
    &amp;= \sigma - \mathbf{y}.
\end{align}\]

<p>The \(\log\) and the devision operates component-wise and</p>

\[\text{diag}\left(\sigma\right) =
\begin{bmatrix} 
    \sigma(\mathbf{o})_1 &amp; 0 &amp; \ldots &amp; 0  \\
    0 &amp; \sigma(\mathbf{o})_2 &amp; \ldots &amp; 0 \\
    \ldots &amp; \ldots &amp; \ldots &amp; \ldots \\
    0 &amp; 0 &amp; \ldots &amp; \sigma(\mathbf{o})_m
\end{bmatrix}\]

<p>holds.
We have to apply the <em>chain rule</em> once again to finally get the desired update values for our weight matrix \(\mathbf{W}\):</p>

\[\begin{align}
\nabla_{\mathbf{W}} L(\mathbf{y}, \sigma) &amp;= \nabla_{\mathbf{o}} L(\mathbf{y}, \sigma) \cdot \nabla_{\mathbf{W}} \mathbf{o} \\
&amp;= (\sigma - \mathbf{y}) \cdot \nabla_{\mathbf{W}} (\mathbf{x} \cdot \mathbf{W})\\
&amp;= (\sigma - \mathbf{y}) \cdot \mathbf{x}^{\top}.
\end{align}\]

<p>Given that \(\sigma\) represents probabilities, and \(\mathbf{y}\) contains only zeros except for one instance of 1 at the position of the “correct” probability, the entries of the \(j^\text{th}\) row (\(x_j=1\)) of the gradient is \(p_i\) if the \(i^\text{th}\) probability is deemed “incorrect”, and \((p_i-1)\) otherwise. 
All other entries are zero.
Note also that \(\mathbf{x}\) is also a one-hot encoded vector.
Consequently, if a probability is correct, it gets increased by \(1-p_i\) and decreased by \(p_i-1\) otherwise.
Therefore, probabilities that are more incorrect experience a larger increase or decrease.</p>

<p>We can actually check this result!
Using the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># use only 1 data point
</span><span class="n">xs</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">ys</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># one-hot-encoding
</span><span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">)).</span><span class="nb">float</span><span class="p">()</span>

<span class="c1"># reinitiate W
</span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">)),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">exp</span><span class="p">()</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">ys</span><span class="p">].</span><span class="n">log</span><span class="p">().</span><span class="n">mean</span><span class="p">()</span>
    
<span class="c1"># backward pass
</span><span class="n">W</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span> 
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y</span><span class="p">[</span><span class="n">ys</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># same
</span><span class="k">print</span><span class="p">(</span><span class="n">xenc</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">probs</span><span class="o">-</span><span class="n">y</span><span class="p">))</span> <span class="c1"># same
</span><span class="k">print</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">grad</span> <span class="o">==</span> <span class="n">xenc</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">probs</span><span class="o">-</span><span class="n">y</span><span class="p">))</span> <span class="c1"># all true
</span></code></pre></div></div>

<h4 id="batching">Batching:</h4>

<p>So far we only considered the math using a single data point \(\mathbf{x}\).
Let us consider a batch of points, i.e.,</p>

\[\mathbf{O} = \mathbf{X}\mathbf{W} = \begin{bmatrix} \mathbf{x}_1 \\ \mathbf{x}_2 \\ \vdots \\ \mathbf{x}_n \end{bmatrix} \mathbf{W}\]

<p>The <em>softmax</em> is still a vector-to-vector transformation, but it’s applied independently to each row of $\mathbf{X}$:</p>

\[\mathbf{S} = \begin{bmatrix} \sigma(\mathbf{o}_1) \\ \sigma(\mathbf{o}_2) \\ \vdots \\ \sigma(\mathbf{o}_n) \end{bmatrix}\]

<p>We can do the exact same steps but I will skip this part.
For the interested reader I refer to <a href="https://mattpetersen.github.io/softmax-with-cross-entropy"></a></p>

<p>Important is that</p>

\[\mathbf{J}_\mathbf{O}(L) = \mathbf{J}_\mathbf{S}(L) \mathbf{J}_\mathbf{O}(S) = \frac{1}{n} \left( \mathbf{S} - \mathbf{Y} \right)\]

<p>and</p>

\[\mathbf{J}_\mathbf{W}(L) = \mathbf{J}_\mathbf{S}(L) \mathbf{J}_\mathbf{O}(S) \mathbf{J}_\mathbf{W}(O) = \frac{1}{n} \left( \mathbf{S} - \mathbf{Y} \right) \mathbf{X}.\]

<p>We can also check this result:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># one-hot-encoding
</span><span class="n">xenc</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">)).</span><span class="nb">float</span><span class="p">()</span>

<span class="c1"># reinitiate W
</span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">)),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> 

<span class="n">logits</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">exp</span><span class="p">()</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">counts</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">ys</span><span class="p">].</span><span class="n">log</span><span class="p">().</span><span class="n">mean</span><span class="p">()</span>
    
<span class="c1"># backward pass
</span><span class="n">W</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span> 
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">)),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span><span class="n">ys</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">print</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># same
</span><span class="k">print</span><span class="p">(</span><span class="n">xenc</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">probs</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">))</span> <span class="c1"># same
</span><span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">xenc</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">probs</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)))</span> <span class="c1"># true
</span></code></pre></div></div>

<h3 id="further-considerations">Further Considerations</h3>

<p>Now, the natural question is: what is the best possible performance we could achieve? 
The answer is that we should aim to match the performance of the <em>Markov chain</em>. 
Indeed, as the process continues, we should expect that the matrix <code class="language-plaintext highlighter-rouge">W</code> will gradually converge towards <code class="language-plaintext highlighter-rouge">P</code>.</p>

<p>Moreover, we should not anticipate surpassing the results achieved by our <em>Markov chain</em> even if we deepen our network, that is, by introducing some <em>hidden layers</em>.
A very good example of useful informations are described in <a class="citation" href="#johnson:2017">(Johnson, 2017)</a>, I discussed in <a href="/Pages/2023/04/02/musical-interrogation-I.html">Part I</a> of this series.
For instance, Johnson adds (compare his interesting <a href="https://www.danieldjohnson.com/2015/08/03/composing-music-with-recurrent-neural-networks/">Blog post</a>)</p>

<ul>
  <li><strong>Positional:</strong> note within the score (that is what we use)</li>
  <li><strong>Pitchclass:</strong> one of the twelve classes</li>
  <li><strong>Previous vicinity:</strong> surrounding notes where played or aticulated last timestep (only useful for polyphonic music)</li>
  <li><strong>Previous context:</strong> the amount of C’s, A’s and so on are played the last timestep (only useful for polyphonic music)</li>
  <li><strong>Beat:</strong> a binary representation of position within the measure</li>
</ul>

<p>However, our expectations may shift if we modify the input, referring to the data that the network processes. 
That being said, we could enhance the training duration.
For instance, introducing a <em>hidden layer</em> results in a loss of <code class="language-plaintext highlighter-rouge">2.693</code> after 2000 epochs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">)</span><span class="o">//</span><span class="mi">4</span><span class="p">),</span> 
                 <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">)</span><span class="o">//</span><span class="mi">4</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">stoi</span><span class="p">)),</span> 
                 <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">epochs</span> <span class="o">=</span> <span class="mi">2_000</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># forward pass
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">xenc</span> <span class="o">@</span> <span class="n">W1</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W2</span>
    <span class="n">odds</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">exp</span><span class="p">()</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">odds</span> <span class="o">/</span> <span class="n">odds</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">probs</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">ys</span><span class="p">].</span><span class="n">log</span><span class="p">().</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'epoch </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    
    <span class="c1"># backward pass
</span>    <span class="n">W1</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># set gradients to zero
</span>    <span class="n">W2</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># set gradients to zero
</span>    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    
    <span class="c1"># update
</span>    <span class="n">W1</span><span class="p">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">10.0</span> <span class="o">*</span> <span class="n">W1</span><span class="p">.</span><span class="n">grad</span>
    <span class="n">W2</span><span class="p">.</span><span class="n">data</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">10.0</span> <span class="o">*</span> <span class="n">W2</span><span class="p">.</span><span class="n">grad</span>
</code></pre></div></div>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="oore:2018">Oore, S., Simon, I., Dieleman, S., Eck, D., &amp; Simonyan, K. (2018). <i>This time with feeling: Learning expressive musical performance</i>.</span></li>
<li><span id="johnson:2017">Johnson, D. D. (2017). Generating polyphonic music using tied parallel networks. <i>EvoMUSART</i>.</span></li></ol>

  </div>

  <div class="PageNavigation">
    
    <a class="prev" href="/Pages/2023/04/23/wekinator.html">&laquo; Replacing Code with ML Models</a>
    
    
    <a class="next" href="/Pages/2023/10/07/system-theory-and-ai.html">Why Machines (Probably) Do Not Think &raquo;</a>
    
  </div><a class="u-url" href="/Pages/2023/05/31/musical-interrogation-II.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Pages/"></data>

  <div class="wrapper">
    <div class="footer-col-wrapper" style="float:right;">
      <div class="footer-col" style="text-align: right;">
        <ul style="list-style:none;  line-height: 50%">
        <li>
        <p class="feed-subscribe">
            <svg class="svg-icon orange">
              <use xlink:href="/Pages/assets/minima-social-icons.svg#rss"></use>
            </svg><a style="color:#999"
              href="https://bzoennchen.github.io/Pages/feed.xml">Subscribe</a>
        </p>
        </li>
        <li>
        <span style="font-size:14px;">
        &#169; 2022. All rights reserved.
        </span>
        </li>
        </ul>
      <!--
        <ul class="contact-list">
          <li class="p-name">Benedikt Zönnchen</li>
          <li><a class="u-email" href="mailto:benedikt.zoennchen@web.de">benedikt.zoennchen@web.de</a></li>
        </ul>-->
      </div>
      <!-- <div class="footer-col">
        <p>A blog dedicated to computer science, education, music, philosophy and technology</p>
      </div> -->
    </div>

    <div class="social-links" style="float:left;"><ul class="social-media-list"><li>
  <a rel="me" href="https://mastodon.social/@BZoennchen" target="_blank" title="Mastodon">
    <svg class="svg-icon grey">
      <use xlink:href="/Pages/assets/minima-social-icons.svg#mastodon"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/BZoennchen" target="_blank" title="GitHub">
    <svg class="svg-icon grey">
      <use xlink:href="/Pages/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://www.youtube.com/channel/UCFqv61UVSmI0h_az5cLV5gQ" target="_blank" title="YouTube">
    <svg class="svg-icon grey">
      <use xlink:href="/Pages/assets/minima-social-icons.svg#youtube"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://scholar.google.de/citations?user=itB89wUAAAAJ" target="_blank" title="Scholar">
    <svg class="svg-icon grey">
      <use xlink:href="/Pages/assets/minima-social-icons.svg#google_scholar"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://orcid.org/0000-0002-0764-2669" target="_blank" title="ORCIC">
    <svg class="svg-icon grey">
      <use xlink:href="/Pages/assets/minima-social-icons.svg#orcid"></use>
    </svg>
  </a>
</li>
</ul></div>
  </div>

</footer></body>

</html>